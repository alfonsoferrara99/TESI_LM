{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4fabbd6-8483-4f1b-9370-84b8bafa0b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pandas/core/computation/expressions.py:22: UserWarning: Pandas requires version '2.10.2' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Using MoE backend 'grouped_mm'\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "‚úÖ Ambiente Pronto e Pulito.\n",
      "   ‚Ä¢ GPU: Tesla V100S-PCIE-32GB\n",
      "   ‚Ä¢ PyTorch: 2.10.0+cu128\n",
      "   ‚Ä¢ Unsloth: 2026.2.1\n",
      "   ‚Ä¢ Transformers: 4.57.6\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELLA 0: SETUP TOTALE (MINIMAL & STABILE)\n",
    "# ==============================================================================\n",
    "import sys\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# 1. BLOCCO MODULI PROBLEMATICI\n",
    "sys.modules[\"vllm\"] = None\n",
    "sys.modules[\"vllm.sampling_params\"] = None\n",
    "\n",
    "print(\"‚è≥ Setup Ambiente in corso... (Attendere, output nascosto)\")\n",
    "\n",
    "# 2. INSTALLAZIONE & AGGIORNAMENTO SILENZIOSO\n",
    "# Scarica l'ultima versione di Unsloth da Git e aggiorna automaticamente \n",
    "# PyTorch e Transformers alle versioni pi√π recenti e compatibili.\n",
    "!pip install --upgrade --no-cache-dir --quiet \\\n",
    "    \"torch\" \"torchvision\" \"torchaudio\" \\\n",
    "    \"transformers\" \"trl\" \"peft\" \"accelerate\" \"bitsandbytes\" \\\n",
    "    \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" \\\n",
    "    \"unsloth_zoo @ git+https://github.com/unslothai/unsloth-zoo.git\" \\\n",
    "    \"pillow\" \"scikit-learn\" \"pandas\"\n",
    "\n",
    "# 3. VERIFICA E PULIZIA\n",
    "clear_output()\n",
    "\n",
    "import torch\n",
    "import unsloth\n",
    "import transformers\n",
    "from PIL import Image\n",
    "\n",
    "print(f\"‚úÖ Ambiente Pronto e Pulito.\")\n",
    "print(f\"   ‚Ä¢ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"   ‚Ä¢ PyTorch: {torch.__version__}\")\n",
    "print(f\"   ‚Ä¢ Unsloth: {unsloth.__version__}\")\n",
    "print(f\"   ‚Ä¢ Transformers: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fa92d33-fc36-493f-8bb4-1efbc25a6a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Verifica librerie...\n",
      "‚úÖ Setup completato. GPU: Tesla V100S-PCIE-32GB\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELLA 1: SETUP, IMPORT & PATHS\n",
    "# ==============================================================================\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "from datasets import load_from_disk\n",
    "from unsloth import FastVisionModel\n",
    "\n",
    "# Setup Display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "# Installazione dipendenze\n",
    "print(\"‚è≥ Verifica librerie...\")\n",
    "try:\n",
    "    import unsloth\n",
    "    from qwen_vl_utils import process_vision_info \n",
    "    import seaborn as sns\n",
    "except ImportError:\n",
    "    !pip install --upgrade --no-cache-dir --quiet \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" \"scikit-learn\" \"pandas\" \"unsloth_zoo\" \"qwen-vl-utils\" \"seaborn\"\n",
    "    from qwen_vl_utils import process_vision_info\n",
    "    import seaborn as sns\n",
    "\n",
    "# Cartella Output Finale\n",
    "TEST_ID = \"TEST_N3_PIPELINE_FULL\"\n",
    "BASE_ROOT = \"TEST_EXPERIMENTS\"\n",
    "RESULTS_DIR = os.path.join(BASE_ROOT, TEST_ID)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Setup completato. GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26212068-673d-44b1-b88d-f51d5f3e9042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Configurazione Pipeline Pronta.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELLA 2: CONFIGURAZIONE PIPELINE & SORGENTI DATI\n",
    "# ==============================================================================\n",
    "SEEDS_TO_TEST = [101, 285, 3692, 92]\n",
    "\n",
    "# --- 1. FONTI DATI ---\n",
    "# Dataset HF (Per gli INPUT: Immagini e Testo)\n",
    "DATASET_HF_PATH = os.path.join(\"DATASET_ITA\", \"PROCESSED_DATA\", \"HF_DATASETS\", \"M1_detection\") \n",
    "\n",
    "# CSV Originale (Per la GROUND TRUTH: Label reali 0-4)\n",
    "CSV_GT_PATH = os.path.join(\"DATASET_ITA\", \"PROCESSED_DATA\", \"splits\", \"master_test.csv\")\n",
    "\n",
    "# --- 2. INPUT M1 (Predizioni cached) ---\n",
    "M1_RESULTS_DIR = os.path.join(BASE_ROOT, \"TEST_N1_M1_BINARY\", \"Qwen2.5-VL-M1-Detection\")\n",
    "\n",
    "# --- 3. MODELLO M2 (Adapter da caricare) ---\n",
    "TRAIN_OUTPUTS_ROOT = \"outputs\" \n",
    "M2_MODEL_SHORTNAME = \"Qwen2.5-VL-M2-Classification\"\n",
    "\n",
    "# --- 4. PARAMETRI ---\n",
    "TARGET_CLASSES_PIPELINE = [\"0\", \"1\", \"2\", \"3\", \"4\"] \n",
    "MODEL_ID_BASE = \"unsloth/Qwen2.5-VL-7B-Instruct-bnb-4bit\"\n",
    "MAX_NEW_TOKENS = 8          \n",
    "TEMPERATURE = 0.0 \n",
    "\n",
    "# --- 5. SYSTEM PROMPT M2 ---\n",
    "SYSTEM_INSTRUCTION_M2 = \"\"\"Sei un classificatore esperto specializzato nella tipologia di contenuti offensivi online.\n",
    "Il contenuto che analizzerai (testo del commento e frame del video) √® GIA' stato identificato come offensivo.\n",
    "Il tuo compito √® classificare ESATTAMENTE il tipo di offesa in una delle seguenti 4 categorie:\n",
    "\n",
    "1. FLAMING: Insulti diretti, linguaggio ostile, aggressivit√† verbale, minacce, uso di parolacce contro una persona.\n",
    "2. DENIGRATION: Attacchi alla reputazione, ridicolizzazione, svalutazione, diffamazione o umiliazione pubblica.\n",
    "3. SEXUAL: Molestie sessuali, commenti lascivi, oggettivazione sessuale, riferimenti espliciti non consensuali.\n",
    "4. RACISM: Discriminazione, stereotipi o insulti basati su razza, etnia, nazionalit√†, religione o colore della pelle.\n",
    "\n",
    "Analizza CONGIUNTAMENTE il testo del commento e i frame del video.\n",
    "Scegli la categoria che meglio descrive l'offesa predominante.\n",
    "Se pi√π categorie sono presenti, scegli quella DOMINANTE.\n",
    "\n",
    "Formato di Output OBBLIGATORIO:\n",
    "Rispondi SOLAMENTE con il numero della classe (1, 2, 3 o 4).\n",
    "Non aggiungere spiegazioni, punteggiatura o testo extra.\"\"\"\n",
    "\n",
    "print(f\"‚öôÔ∏è Configurazione Pipeline Pronta.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d537b141-586b-4fe7-8c7a-c6b5dd092b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Caricamento Risorse...\n",
      "   ‚úÖ Dataset HF caricato: 855 campioni.\n",
      "   ‚úÖ CSV Master caricato: 855 righe.\n",
      "\n",
      "üîç Eseguo SANITY CHECK sull'allineamento (HF vs CSV)...\n",
      "   üìä Match Rate: 100.00% (855/855)\n",
      "   ‚úÖ ALLINEAMENTO ACCETTABILE. Procedo per indice.\n",
      "‚úÖ Setup Dati Completato.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELLA 3: DATASET & SANITY CHECK\n",
    "# ==============================================================================\n",
    "import re\n",
    "\n",
    "print(\"üìÇ Caricamento Risorse...\")\n",
    "\n",
    "# 1. Caricamento HF\n",
    "try:\n",
    "    dataset_raw = load_from_disk(DATASET_HF_PATH)\n",
    "    test_dataset = dataset_raw[\"test\"] if \"test\" in dataset_raw else dataset_raw[\"val\"]\n",
    "    print(f\"   ‚úÖ Dataset HF caricato: {len(test_dataset)} campioni.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Errore caricamento HF: {e}\")\n",
    "\n",
    "# 2. Caricamento CSV GT\n",
    "try:\n",
    "    df_gt = pd.read_csv(CSV_GT_PATH, sep=';', dtype={'video_id': str}) \n",
    "    print(f\"   ‚úÖ CSV Master caricato: {len(df_gt)} righe.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Errore caricamento CSV: {e}\")\n",
    "\n",
    "# 3. Allineamento Lunghezze\n",
    "if len(test_dataset) != len(df_gt):\n",
    "    print(f\"‚ö†Ô∏è DISALLINEAMENTO LUNGHEZZE: HF={len(test_dataset)} vs CSV={len(df_gt)}\")\n",
    "    min_len = min(len(test_dataset), len(df_gt))\n",
    "    test_dataset = test_dataset.select(range(min_len))\n",
    "    df_gt = df_gt.iloc[:min_len]\n",
    "    print(f\"   ‚úÇÔ∏è Troncato dataset a {min_len} righe.\")\n",
    "\n",
    "# --- SANITY CHECK ROBUSTO ---\n",
    "print(\"\\nüîç Eseguo SANITY CHECK sull'allineamento (HF vs CSV)...\")\n",
    "\n",
    "def normalize_simple(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    # Rimuoviamo formattazione, virgolette e punteggiatura varia per il confronto\n",
    "    clean = text.replace(\"Commento:\", \"\").replace('\"', '').replace(\"'\", \"\").strip().lower()\n",
    "    return re.sub(r'[\\W_]+', '', clean) # Solo caratteri alfanumerici uniti\n",
    "\n",
    "matches = 0\n",
    "check_limit = len(test_dataset)\n",
    "\n",
    "for i in range(check_limit):\n",
    "    # --- ESTRAZIONE ROBUSTA TESTO HF ---\n",
    "    hf_text_raw = \"\"\n",
    "    try:\n",
    "        for msg in test_dataset[i][\"messages\"]:\n",
    "            if msg[\"role\"] == \"user\":\n",
    "                for item in msg[\"content\"]:\n",
    "                    if item[\"type\"] == \"text\":\n",
    "                        hf_text_raw = item[\"text\"]\n",
    "                        break\n",
    "    except (KeyError, TypeError, IndexError):\n",
    "        hf_text_raw = \"\"\n",
    "\n",
    "    # --- ESTRAZIONE TESTO CSV ---\n",
    "    csv_text_raw = str(df_gt.iloc[i]['Comment']) if i < len(df_gt) else \"\"\n",
    "    \n",
    "    # --- CONFRONTO ---\n",
    "    if normalize_simple(hf_text_raw) == normalize_simple(csv_text_raw):\n",
    "        matches += 1\n",
    "    else:\n",
    "        # Stampiamo solo il primo errore per capire cosa succede\n",
    "        if matches == i: \n",
    "            print(f\"   ‚ùå Mismatch all'indice {i}:\")\n",
    "            print(f\"      HF (raw):  {repr(hf_text_raw[:100])}\")\n",
    "            print(f\"      CSV (raw): {repr(csv_text_raw[:100])}\")\n",
    "\n",
    "match_rate = matches / check_limit\n",
    "print(f\"   üìä Match Rate: {match_rate:.2%} ({matches}/{check_limit})\")\n",
    "\n",
    "if match_rate < 0.95: # Tolleranza leggermente pi√π alta per via di encoding/emoji\n",
    "    print(\"‚õî ERRORE: L'ordine √® sballato o il testo √® troppo diverso.\")\n",
    "    # Se vuoi forzare lo stop scommenta la riga sotto:\n",
    "    # raise ValueError(\"Allineamento fallito.\")\n",
    "else:\n",
    "    print(\"   ‚úÖ ALLINEAMENTO ACCETTABILE. Procedo per indice.\")\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "def get_gt_by_index(idx):\n",
    "    raw_type = str(df_gt.iloc[idx]['Type']).strip().lower()\n",
    "    mapping = {\n",
    "        \"none\": \"0\", \"0\": \"0\", \"neutral\": \"0\",\n",
    "        \"flaming\": \"1\", \"1\": \"1\",\n",
    "        \"denigration\": \"2\", \"2\": \"2\",\n",
    "        \"sexual\": \"3\", \"3\": \"3\",\n",
    "        \"racism\": \"4\", \"4\": \"4\"\n",
    "    }\n",
    "    return mapping.get(raw_type, \"0\")\n",
    "\n",
    "def get_m1_predictions_map(seed_or_baseline):\n",
    "    if seed_or_baseline == \"baseline\": filename = \"predictions_baseline.csv\"\n",
    "    else: filename = f\"predictions_seed_{seed_or_baseline}.csv\"\n",
    "    path = os.path.join(M1_RESULTS_DIR, filename)\n",
    "    if not os.path.exists(path): return {}\n",
    "    df = pd.read_csv(path)\n",
    "    return dict(zip(df[\"sample_idx\"], df[\"pred_label\"].astype(str)))\n",
    "\n",
    "print(\"‚úÖ Setup Dati Completato.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5455794c-a73c-4f77-b500-e8a107a94975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELLA 4: HELPER PER COSTRUIRE INPUT M2 (CON CHECK IMMAGINI)\n",
    "# ==============================================================================\n",
    "def build_input_for_m2(sample):\n",
    "    \"\"\"\n",
    "    Prepara l'input per M2.\n",
    "    Restituisce: (prompt_msgs, has_missing_images_flag)\n",
    "    \"\"\"\n",
    "    user_content_raw = None\n",
    "    for msg in sample[\"messages\"]:\n",
    "        if msg[\"role\"] == \"user\":\n",
    "            user_content_raw = msg[\"content\"]\n",
    "            break\n",
    "            \n",
    "    if not user_content_raw: return None, False\n",
    "\n",
    "    final_user_content = []\n",
    "    has_missing_images = False\n",
    "    \n",
    "    for item in user_content_raw:\n",
    "        if item[\"type\"] == \"image\":\n",
    "            raw_path = item[\"image\"]\n",
    "            clean_path = raw_path.replace(\"file://\", \"\")\n",
    "            clean_path = \"/\" + clean_path.lstrip(\"/\")\n",
    "            \n",
    "            if os.path.exists(clean_path):\n",
    "                final_user_content.append({\"type\": \"image\", \"image\": f\"file://{clean_path}\"})\n",
    "            else:\n",
    "                # Segnaliamo che un'immagine prevista non √® stata trovata\n",
    "                has_missing_images = True\n",
    "                \n",
    "        elif item[\"type\"] == \"text\":\n",
    "            text_nude = item[\"text\"].replace(\"Commento:\", \"\").strip().strip('\"').strip(\"'\")\n",
    "            final_user_content.append({\"type\": \"text\", \"text\": f\"Commento: \\\"{text_nude}\\\"\"})\n",
    "\n",
    "    prompt_msgs = [\n",
    "        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": SYSTEM_INSTRUCTION_M2}]},\n",
    "        {\"role\": \"user\", \"content\": final_user_content}\n",
    "    ]\n",
    "    return prompt_msgs, has_missing_images\n",
    "\n",
    "def parse_m2_prediction(pred_text):\n",
    "    clean = pred_text.strip()\n",
    "    if clean in [\"1\", \"2\", \"3\", \"4\"]: return clean\n",
    "    matches = re.findall(r'\\b(1|2|3|4)\\b', clean)\n",
    "    return matches[0] if matches else \"INVALID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "319dd75d-46c8-413e-82cc-fe3c3cdf86e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELLA 5: PIPELINE ENGINE (AUDIT LOGGING & RELIABILITY CHECK)\n",
    "# ==============================================================================\n",
    "def run_pipeline_test(m2_model, m2_tokenizer, m1_preds_map, dataset, desc=\"Pipeline\"):\n",
    "    \n",
    "    if m2_model: FastVisionModel.for_inference(m2_model)\n",
    "    \n",
    "    final_predictions = []\n",
    "    ground_truth_5class = []\n",
    "    \n",
    "    # --- AUDIT COUNTERS (Per onest√† scientifica) ---\n",
    "    stats = {\n",
    "        \"m2_calls\": 0,              # Quante volte M1 ha detto \"1\"\n",
    "        \"m1_missing\": 0,            # M1 non aveva output per questo sample\n",
    "        \"m2_input_empty\": 0,        # M2 non ha ricevuto input valido (es. no text/img)\n",
    "        \"m2_invalid_output\": 0,     # M2 ha risposto \"blabla\" invece di 1-4\n",
    "        \"m2_missing_images\": 0,     # M2 ha girato solo col testo (img mancante)\n",
    "        \"fallback_events\": 0        # Totale volte che abbiamo \"inventato\" la risposta\n",
    "    }\n",
    "    \n",
    "    print(f\"üöÄ Avvio Pipeline: {desc}\")\n",
    "    \n",
    "    for idx, sample in enumerate(tqdm(dataset, desc=desc)):\n",
    "        # 1. Ground Truth\n",
    "        gt_5 = get_gt_by_index(idx)\n",
    "        ground_truth_5class.append(gt_5)\n",
    "        \n",
    "        # 2. M1 Output\n",
    "        m1_out = str(m1_preds_map.get(idx, \"MISSING\"))\n",
    "        \n",
    "        if m1_out == \"MISSING\":\n",
    "            stats[\"m1_missing\"] += 1\n",
    "            final_predictions.append(\"0\") # Fallback conservativo (Safe)\n",
    "            continue\n",
    "\n",
    "        # 3. Pipeline Logic\n",
    "        if m1_out == \"0\":\n",
    "            final_predictions.append(\"0\")\n",
    "        else:\n",
    "            stats[\"m2_calls\"] += 1\n",
    "            \n",
    "            # Input M2\n",
    "            prompt_msgs, missing_imgs = build_input_for_m2(sample)\n",
    "            if missing_imgs: stats[\"m2_missing_images\"] += 1\n",
    "            \n",
    "            if prompt_msgs is None:\n",
    "                stats[\"m2_input_empty\"] += 1\n",
    "                stats[\"fallback_events\"] += 1\n",
    "                final_predictions.append(\"2\") # Fallback tecnico\n",
    "                continue\n",
    "                \n",
    "            # Inferenza M2\n",
    "            image_inputs, video_inputs = process_vision_info(prompt_msgs)\n",
    "            text = m2_tokenizer.apply_chat_template(prompt_msgs, tokenize=False, add_generation_prompt=True)\n",
    "            inputs = m2_tokenizer(text=[text], images=image_inputs, videos=video_inputs, padding=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = m2_model.generate(**inputs, max_new_tokens=8, use_cache=True, temperature=0, do_sample=False)\n",
    "                \n",
    "            pred_text = m2_tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "            m2_out = parse_m2_prediction(pred_text)\n",
    "            \n",
    "            if m2_out == \"INVALID\":\n",
    "                stats[\"m2_invalid_output\"] += 1\n",
    "                stats[\"fallback_events\"] += 1\n",
    "                final_predictions.append(\"2\") # Fallback tecnico\n",
    "            else:\n",
    "                final_predictions.append(m2_out)\n",
    "\n",
    "    # --- METRICHE ---\n",
    "    acc = accuracy_score(ground_truth_5class, final_predictions)\n",
    "    p_macro, r_macro, f1_macro, _ = precision_recall_fscore_support(ground_truth_5class, final_predictions, average='macro', zero_division=0)\n",
    "    cm = confusion_matrix(ground_truth_5class, final_predictions, labels=TARGET_CLASSES_PIPELINE)\n",
    "    \n",
    "    # Calcolo percentuali audit\n",
    "    total = len(dataset)\n",
    "    audit_report = {\n",
    "        \"m2_activation_rate\": stats[\"m2_calls\"] / total,\n",
    "        \"reliability_risk\": stats[\"fallback_events\"] / max(1, stats[\"m2_calls\"]), # % di M2 calls che sono fallback\n",
    "        \"missing_images_rate\": stats[\"m2_missing_images\"] / max(1, stats[\"m2_calls\"]),\n",
    "        \"raw_stats\": stats\n",
    "    }\n",
    "    \n",
    "    results = {\n",
    "        \"accuracy\": acc, \"f1_macro\": f1_macro, \"precision_macro\": p_macro, \"recall_macro\": r_macro,\n",
    "        \"audit\": audit_report,\n",
    "        \"conf_matrix\": cm.tolist()\n",
    "    }\n",
    "    \n",
    "    # Per-Class\n",
    "    p, r, f1, _ = precision_recall_fscore_support(ground_truth_5class, final_predictions, average=None, labels=TARGET_CLASSES_PIPELINE, zero_division=0)\n",
    "    for i, c in enumerate(TARGET_CLASSES_PIPELINE):\n",
    "        results[f\"class_{c}_p\"] = p[i]\n",
    "        results[f\"class_{c}_r\"] = r[i]\n",
    "        results[f\"class_{c}_f1\"] = f1[i]\n",
    "        \n",
    "    return results, final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45ee3501-2819-4d38-bbec-425b01f373f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîµ TEST BASELINE PIPELINE...\n",
      "==((====))==  Unsloth 2026.2.1: Fast Qwen2_5_Vl patching. Transformers: 4.57.6. vLLM: 0.6.3.\n",
      "   \\\\   /|    Tesla V100S-PCIE-32GB. Num GPUs = 1. Max memory: 31.739 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 7.0. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "üöÄ Avvio Pipeline: Pipeline Baseline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pipeline Baseline: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 855/855 [00:19<00:00, 43.09it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üü£ TEST PIPELINE SEED 101...\n",
      "==((====))==  Unsloth 2026.2.1: Fast Qwen2_5_Vl patching. Transformers: 4.57.6. vLLM: 0.6.3.\n",
      "   \\\\   /|    Tesla V100S-PCIE-32GB. Num GPUs = 1. Max memory: 31.739 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 7.0. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "üöÄ Avvio Pipeline: Pipe Seed 101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pipe Seed 101: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 855/855 [01:37<00:00,  8.80it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üü£ TEST PIPELINE SEED 285...\n",
      "==((====))==  Unsloth 2026.2.1: Fast Qwen2_5_Vl patching. Transformers: 4.57.6. vLLM: 0.6.3.\n",
      "   \\\\   /|    Tesla V100S-PCIE-32GB. Num GPUs = 1. Max memory: 31.739 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 7.0. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "üöÄ Avvio Pipeline: Pipe Seed 285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pipe Seed 285: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 855/855 [02:27<00:00,  5.80it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üü£ TEST PIPELINE SEED 3692...\n",
      "==((====))==  Unsloth 2026.2.1: Fast Qwen2_5_Vl patching. Transformers: 4.57.6. vLLM: 0.6.3.\n",
      "   \\\\   /|    Tesla V100S-PCIE-32GB. Num GPUs = 1. Max memory: 31.739 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 7.0. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "üöÄ Avvio Pipeline: Pipe Seed 3692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pipe Seed 3692: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 855/855 [02:20<00:00,  6.10it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üü£ TEST PIPELINE SEED 92...\n",
      "==((====))==  Unsloth 2026.2.1: Fast Qwen2_5_Vl patching. Transformers: 4.57.6. vLLM: 0.6.3.\n",
      "   \\\\   /|    Tesla V100S-PCIE-32GB. Num GPUs = 1. Max memory: 31.739 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 7.0. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "üöÄ Avvio Pipeline: Pipe Seed 92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pipe Seed 92: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 855/855 [02:33<00:00,  5.56it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Pipeline Testing Completato.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELLA 6: ESECUZIONE TEST CON RECUPERO METADATI TEMPI\n",
    "# ==============================================================================\n",
    "import glob\n",
    "\n",
    "def get_train_time_from_file(model_shortname, seed):\n",
    "    \"\"\"Cerca il JSON di training per estrarre i minuti.\"\"\"\n",
    "    path = os.path.join(TRAIN_OUTPUTS_ROOT, f\"{model_shortname}_Seed_{seed}\", f\"training_report_Seed_{seed}.json\")\n",
    "    try:\n",
    "        with open(path, 'r') as f: return json.load(f)[\"4_TRAINING_PERFORMANCE\"][\"total_duration_minutes\"]\n",
    "    except: return 0.0\n",
    "\n",
    "def get_inference_time_from_test(test_dir, seed):\n",
    "    \"\"\"Cerca il JSON di test per estrarre i secondi di inferenza.\"\"\"\n",
    "    path = os.path.join(test_dir, f\"results_seed_{seed}.json\")\n",
    "    try:\n",
    "        with open(path, 'r') as f: return json.load(f)[\"avg_inference_time_sec\"]\n",
    "    except: return 0.0\n",
    "\n",
    "pipeline_results = []\n",
    "M1_TEST_DIR = os.path.join(BASE_ROOT, \"TEST_N1_M1_BINARY\", \"Qwen2.5-VL-M1-Detection\")\n",
    "M2_TEST_DIR = os.path.join(BASE_ROOT, \"TEST_N2_M2_MULTICLASS\", \"Qwen2.5-VL-M2-Classification\")\n",
    "\n",
    "# --- BASELINE ---\n",
    "m1_base_map = get_m1_predictions_map(\"baseline\")\n",
    "if m1_base_map:\n",
    "    print(\"\\nüîµ TEST BASELINE PIPELINE...\")\n",
    "    model, tokenizer = FastVisionModel.from_pretrained(MODEL_ID_BASE, load_in_4bit=True)\n",
    "    res_base, _ = run_pipeline_test(model, tokenizer, m1_base_map, test_dataset, desc=\"Pipeline Baseline\")\n",
    "    res_base.update({\"model\": \"Baseline\", \"seed\": \"N/A\"})\n",
    "    \n",
    "    # Salvataggio\n",
    "    with open(os.path.join(RESULTS_DIR, \"results_baseline.json\"), 'w') as f:\n",
    "        json.dump(res_base, f, indent=4)\n",
    "    del model, tokenizer; gc.collect(); torch.cuda.empty_cache()\n",
    "else:\n",
    "    res_base = {}\n",
    "\n",
    "# --- FINE-TUNED SEEDS ---\n",
    "for seed in SEEDS_TO_TEST:\n",
    "    print(f\"\\nüü£ TEST PIPELINE SEED {seed}...\")\n",
    "    \n",
    "    m1_seed_map = get_m1_predictions_map(seed)\n",
    "    if not m1_seed_map: continue\n",
    "        \n",
    "    adapter_path = os.path.join(TRAIN_OUTPUTS_ROOT, f\"{M2_MODEL_SHORTNAME}_Seed_{seed}\", \"final_adapter_latest\")\n",
    "    if not os.path.exists(adapter_path): continue\n",
    "        \n",
    "    model, tokenizer = FastVisionModel.from_pretrained(model_name=MODEL_ID_BASE, load_in_4bit=True)\n",
    "    model.load_adapter(adapter_path)\n",
    "    \n",
    "    res, _ = run_pipeline_test(model, tokenizer, m1_seed_map, test_dataset, desc=f\"Pipe Seed {seed}\")\n",
    "    \n",
    "    # --- ARRICCHIMENTO DATI TEMPI ---\n",
    "    t_train_m1 = get_train_time_from_file(\"Qwen2.5-VL-M1-Detection\", seed)\n",
    "    t_train_m2 = get_train_time_from_file(\"Qwen2.5-VL-M2-Classification\", seed)\n",
    "    t_inf_m1 = get_inference_time_from_test(M1_TEST_DIR, seed)\n",
    "    t_inf_m2 = get_inference_time_from_test(M2_TEST_DIR, seed)\n",
    "    \n",
    "    res.update({\n",
    "        \"model\": \"Fine-Tuned\", \"seed\": seed,\n",
    "        \"times\": {\n",
    "            \"train_m1_min\": t_train_m1, \"train_m2_min\": t_train_m2,\n",
    "            \"inf_m1_sec\": t_inf_m1, \"inf_m2_sec\": t_inf_m2,\n",
    "            \"total_train_min\": t_train_m1 + t_train_m2\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    pipeline_results.append(res)\n",
    "    with open(os.path.join(RESULTS_DIR, f\"results_seed_{seed}.json\"), 'w') as f:\n",
    "        json.dump(res, f, indent=4)\n",
    "        \n",
    "    del model, tokenizer; gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n‚úÖ Pipeline Testing Completato.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21944dd8-bf12-4d9c-aa90-2b6dd5f78fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Report PDF rigenerato (REV7): TEST_EXPERIMENTS/TEST_N3_PIPELINE_FULL/Report_PIPELINE_FINAL.pdf\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELLA 7: REPORT PDF PIPELINE (REV7 - 3 pagine, zero overlap, CM labels fix)\n",
    "# ==============================================================================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from datetime import datetime\n",
    "from textwrap import fill\n",
    "from matplotlib.patches import FancyBboxPatch, FancyArrowPatch\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities\n",
    "# -----------------------------\n",
    "def safe_get(d, key, default=None):\n",
    "    return d.get(key, default) if isinstance(d, dict) else default\n",
    "\n",
    "def to_float(x):\n",
    "    try:\n",
    "        if x is None: return None\n",
    "        if isinstance(x, (int, float, np.number)): return float(x)\n",
    "        if isinstance(x, str) and x.strip() == \"\": return None\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def mean_std(values, ddof=1):\n",
    "    vals = [to_float(v) for v in values if to_float(v) is not None]\n",
    "    if len(vals) == 0: return None, None, 0\n",
    "    if len(vals) == 1: return float(vals[0]), 0.0, 1\n",
    "    return float(np.mean(vals)), float(np.std(vals, ddof=ddof)), len(vals)\n",
    "\n",
    "def fmt_mean_std(m, s, fmt=\"{:.4f}\", unit=\"\"):\n",
    "    if m is None: return \"N/D\"\n",
    "    if s is None: return (fmt.format(m) + (f\" {unit}\" if unit else \"\")).strip()\n",
    "    return (fmt.format(m) + \" ¬± \" + fmt.format(s) + (f\" {unit}\" if unit else \"\")).strip()\n",
    "\n",
    "def fmt_val(v, fmt=\"{:.4f}\", unit=\"\"):\n",
    "    v = to_float(v)\n",
    "    if v is None: return \"N/D\"\n",
    "    return (fmt.format(v) + (f\" {unit}\" if unit else \"\")).strip()\n",
    "\n",
    "def pct(v, decimals=1):\n",
    "    v = to_float(v)\n",
    "    if v is None: return \"N/D\"\n",
    "    return f\"{v*100:.{decimals}f}%\"\n",
    "\n",
    "def pick_gpu_name():\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            return torch.cuda.get_device_name(0)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return \"N/D\"\n",
    "\n",
    "def add_section_header(ax, title, color=\"#1f4e79\"):\n",
    "    ax.axis(\"off\")\n",
    "    ax.text(0.0, 0.65, title, fontsize=12, fontweight=\"bold\", color=color, va=\"center\", ha=\"left\")\n",
    "    ax.plot([0, 1], [0.08, 0.08], color=\"#d0d0d0\", lw=1)\n",
    "\n",
    "def make_table(ax, data, col_widths=None, header_rows=1, font_size=9, row_scale=1.35,\n",
    "               header_face=\"#e8f1fb\", zebra=True, align_left_cols=None,\n",
    "               body_text_color=\"#000000\", header_text_color=\"#000000\"):\n",
    "    ax.axis(\"off\")\n",
    "    table = ax.table(cellText=data, loc=\"center\", cellLoc=\"center\", colWidths=col_widths)\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(font_size)\n",
    "    table.scale(1, row_scale)\n",
    "    for (r, c), cell in table.get_celld().items():\n",
    "        cell.set_edgecolor(\"#c7c7c7\")\n",
    "        cell.set_linewidth(0.6)\n",
    "        if r < header_rows:\n",
    "            cell.set_facecolor(header_face)\n",
    "            cell.set_text_props(fontweight=\"bold\", color=header_text_color)\n",
    "        else:\n",
    "            if zebra and (r % 2 == 0):\n",
    "                cell.set_facecolor(\"#fafafa\")\n",
    "            cell.set_text_props(color=body_text_color)\n",
    "        if align_left_cols and c in align_left_cols:\n",
    "            cell._loc = \"left\"\n",
    "            cell.PAD = 0.02\n",
    "    return table\n",
    "\n",
    "def extract_time_from_row(row, key):\n",
    "    v = safe_get(row, key, None)\n",
    "    if v is not None: return v\n",
    "    t = safe_get(row, \"times\", None)\n",
    "    if isinstance(t, dict): return t.get(key, None)\n",
    "    return None\n",
    "\n",
    "def extract_audit_from_row(row, key):\n",
    "    v = safe_get(row, key, None)\n",
    "    if v is not None: return v\n",
    "    a = safe_get(row, \"audit\", None)\n",
    "    if isinstance(a, dict): return a.get(key, None)\n",
    "    return None\n",
    "\n",
    "# -----------------------------\n",
    "# Preconditions\n",
    "# -----------------------------\n",
    "required = [\"pipeline_results\", \"res_base\", \"RESULTS_DIR\", \"TARGET_CLASSES_PIPELINE\"]\n",
    "missing = [x for x in required if x not in globals()]\n",
    "if missing:\n",
    "    raise ValueError(f\"Mancano variabili richieste per la reportistica: {missing}\")\n",
    "\n",
    "TARGET_CLASSES_PIPELINE = [str(x) for x in TARGET_CLASSES_PIPELINE]\n",
    "df = pd.DataFrame(pipeline_results)\n",
    "if df.empty:\n",
    "    raise ValueError(\"pipeline_results √® vuoto: impossibile generare report.\")\n",
    "\n",
    "N_TEST = len(test_dataset) if \"test_dataset\" in globals() else None\n",
    "SEEDS_TO_TEST = globals().get(\"SEEDS_TO_TEST\", [safe_get(r, \"seed\", None) for r in pipeline_results if safe_get(r, \"seed\", None) is not None])\n",
    "\n",
    "# Best run per visual\n",
    "if \"f1_macro\" in df.columns:\n",
    "    best_idx = df[\"f1_macro\"].astype(float).idxmax()\n",
    "elif \"accuracy\" in df.columns:\n",
    "    best_idx = df[\"accuracy\"].astype(float).idxmax()\n",
    "else:\n",
    "    best_idx = df.index[0]\n",
    "best_run = df.loc[best_idx].to_dict()\n",
    "best_seed = safe_get(best_run, \"seed\", \"N/D\")\n",
    "\n",
    "# -----------------------------\n",
    "# Stats\n",
    "# -----------------------------\n",
    "overall_keys = [\"accuracy\", \"precision_macro\", \"recall_macro\", \"f1_macro\"]\n",
    "overall_stats = {k: (mean_std(df[k].tolist(), ddof=1) if k in df.columns else (None, None, 0)) for k in overall_keys}\n",
    "\n",
    "per_class_stats = {}\n",
    "for c in TARGET_CLASSES_PIPELINE:\n",
    "    for suffix in [\"p\", \"r\", \"f1\"]:\n",
    "        key = f\"class_{c}_{suffix}\"\n",
    "        per_class_stats[key] = mean_std(df[key].tolist(), ddof=1) if key in df.columns else (None, None, 0)\n",
    "\n",
    "time_keys = [\n",
    "    (\"train_m1_min\", \"Train M1\", \"min\", \"{:.1f}\"),\n",
    "    (\"train_m2_min\", \"Train M2\", \"min\", \"{:.1f}\"),\n",
    "    (\"inf_m1_sec\",   \"Inferenza M1\", \"sec/campione\", \"{:.3f}\"),\n",
    "    (\"inf_m2_sec\",   \"Inferenza M2\", \"sec/campione\", \"{:.3f}\"),\n",
    "]\n",
    "time_stats = []\n",
    "for key, label, unit, fmt in time_keys:\n",
    "    vals = [extract_time_from_row(r, key) for r in pipeline_results]\n",
    "    m, s, n = mean_std(vals, ddof=1)\n",
    "    time_stats.append((key, label, m, s, n, unit, fmt))\n",
    "\n",
    "audit_keys = [\n",
    "    (\"m2_activation_rate\", \"Attivazione M2\", \"quota campioni inviati a M2 (m2_calls / N)\"),\n",
    "    (\"reliability_risk\", \"Fallback/Invalid\", \"quota chiamate M2 finite in fallback (fallback_events / m2_calls)\"),\n",
    "    (\"missing_images_rate\", \"Missing immagini (su M2)\", \"quota chiamate M2 con immagini mancanti (m2_missing_images / m2_calls)\"),\n",
    "]\n",
    "audit_stats = []\n",
    "for key, label, note in audit_keys:\n",
    "    vals = [extract_audit_from_row(r, key) for r in pipeline_results]\n",
    "    m, s, n = mean_std(vals, ddof=1)\n",
    "    audit_stats.append((key, label, m, s, n, note))\n",
    "\n",
    "# Missing pred M1 rate\n",
    "m1_missing_rates = []\n",
    "for r in pipeline_results:\n",
    "    a = safe_get(r, \"audit\", {})\n",
    "    rs = safe_get(a, \"raw_stats\", {})\n",
    "    miss = safe_get(rs, \"m1_missing\", None)\n",
    "    if miss is not None and N_TEST:\n",
    "        m1_missing_rates.append(miss / N_TEST)\n",
    "m1_miss_m, m1_miss_s, _ = mean_std(m1_missing_rates, ddof=1) if m1_missing_rates else (None, None, 0)\n",
    "\n",
    "# -----------------------------\n",
    "# Labels\n",
    "# -----------------------------\n",
    "classes_map = {\"0\": \"Safe\", \"1\": \"Flaming\", \"2\": \"Denigration\", \"3\": \"Sexual\", \"4\": \"Racism\"}\n",
    "labels_long = [classes_map.get(str(i), str(i)) for i in TARGET_CLASSES_PIPELINE]\n",
    "labels_short = [\"Safe\", \"Flaming\", \"Denigr.\", \"Sexual\", \"Racism\"]  # per CM: evita attaccamenti\n",
    "\n",
    "TITLE_COLOR = \"#1f4e79\"\n",
    "SOFT_BG = \"#f3f6fb\"\n",
    "plt.rcParams[\"font.family\"] = \"DejaVu Sans\"\n",
    "\n",
    "# -----------------------------\n",
    "# Pipeline diagram (stilizzato)\n",
    "# -----------------------------\n",
    "def draw_pipeline_diagram(ax):\n",
    "    ax.set_xlim(0, 1); ax.set_ylim(0, 1); ax.axis(\"off\")\n",
    "\n",
    "    def box(x, y, w, h, text, fc, ec=\"#c9d2dc\", lw=1.2, fs=10, bold=False):\n",
    "        p = FancyBboxPatch((x, y), w, h, boxstyle=\"round,pad=0.02,rounding_size=0.03\",\n",
    "                           linewidth=lw, edgecolor=ec, facecolor=fc)\n",
    "        ax.add_patch(p)\n",
    "        ax.text(x+w/2, y+h/2, text, ha=\"center\", va=\"center\",\n",
    "                fontsize=fs, fontweight=(\"bold\" if bold else \"normal\"), color=\"#000000\")\n",
    "        return (x, y, w, h)\n",
    "\n",
    "    def arrow(x1, y1, x2, y2):\n",
    "        ax.add_patch(FancyArrowPatch((x1, y1), (x2, y2),\n",
    "                                     arrowstyle=\"-|>\", mutation_scale=14,\n",
    "                                     linewidth=1.6, color=\"#2f3b46\"))\n",
    "\n",
    "    box(0.05, 0.38, 0.22, 0.26, \"INPUT\\n(Frame + Commento)\", fc=\"#ffffff\", ec=\"#b9c6d3\", bold=True)\n",
    "    box(0.33, 0.44, 0.20, 0.14, \"M1\\nDetection\\n0 / 1*\", fc=\"#e8f1fb\", ec=\"#93b4d8\", bold=True)\n",
    "    box(0.72, 0.62, 0.23, 0.16, \"OUTPUT = 0\\n(Safe)\", fc=\"#f2f2f2\", ec=\"#c9c9c9\", bold=True)\n",
    "    box(0.63, 0.28, 0.20, 0.14, \"M2\\nClassificazione\\n1..4\", fc=\"#eaf7ef\", ec=\"#8bc6a0\", bold=True)\n",
    "    box(0.86, 0.28, 0.12, 0.14, \"MERGE\\n0..4\", fc=\"#fff7e6\", ec=\"#f0d9a8\", bold=True)\n",
    "\n",
    "    arrow(0.27, 0.51, 0.33, 0.51)     # input -> M1\n",
    "    arrow(0.53, 0.54, 0.72, 0.70)     # M1 -> out0\n",
    "    arrow(0.53, 0.46, 0.63, 0.35)     # M1 -> M2\n",
    "    ax.text(0.60, 0.72, \"se M1=0\", fontsize=9, color=\"#000000\")\n",
    "    ax.text(0.57, 0.35, \"se M1=1*\", fontsize=9, color=\"#000000\")\n",
    "    arrow(0.83, 0.35, 0.86, 0.35)     # M2 -> merge\n",
    "    arrow(0.92, 0.28, 0.92, 0.12)     # merge -> final\n",
    "    ax.text(0.92, 0.07, \"OUTPUT finale\\n(0..4)\", ha=\"center\", va=\"center\",\n",
    "            fontsize=10, fontweight=\"bold\", color=\"#000000\")\n",
    "\n",
    "# -----------------------------\n",
    "# Confusion matrix easy (stretta + etichette corte)\n",
    "# -----------------------------\n",
    "def draw_easy_confusion_matrix(ax, cm, row_labels, col_labels, scale_x=0.70, scale_y=1.60, font_size=9):\n",
    "    ax.axis(\"off\")\n",
    "    cm = np.array(cm, dtype=int)\n",
    "    n = cm.shape[0]\n",
    "\n",
    "    cell_text = []\n",
    "    for i in range(n):\n",
    "        row = []\n",
    "        for j in range(n):\n",
    "            row.append(f\"{cm[i,j]} ‚úì\" if i == j else f\"{cm[i,j]}\")\n",
    "        cell_text.append(row)\n",
    "\n",
    "    table = ax.table(cellText=cell_text, rowLabels=row_labels, colLabels=col_labels,\n",
    "                     cellLoc=\"center\", loc=\"center\")\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(font_size)\n",
    "    table.scale(scale_x, scale_y)\n",
    "\n",
    "    bg = \"#111315\"; header_bg = \"#1b1f24\"\n",
    "    text_col = \"#f2f2f2\"; diag_bg = \"#163a2a\"; diag_text = \"#b8f2d3\"; edge = \"#2a2f36\"\n",
    "\n",
    "    for (r, c), cell in table.get_celld().items():\n",
    "        cell.set_edgecolor(edge); cell.set_linewidth(0.8)\n",
    "        if r == 0 or c == -1:\n",
    "            cell.set_facecolor(header_bg)\n",
    "            cell.get_text().set_color(text_col)\n",
    "            cell.get_text().set_fontweight(\"bold\")\n",
    "            continue\n",
    "        i = r - 1; j = c\n",
    "        if i == j:\n",
    "            cell.set_facecolor(diag_bg)\n",
    "            cell.get_text().set_color(diag_text)\n",
    "            cell.get_text().set_fontweight(\"bold\")\n",
    "        else:\n",
    "            cell.set_facecolor(bg)\n",
    "            cell.get_text().set_color(text_col)\n",
    "\n",
    "    ax.text(0.0, 1.03, \"Reale \\\\ Predetto\", transform=ax.transAxes,\n",
    "            ha=\"left\", va=\"bottom\", fontsize=11, fontweight=\"bold\", color=\"#000000\")\n",
    "\n",
    "# -----------------------------\n",
    "# Stabilit√† boxplot P/R/F1 (sintesi sotto, alta abbastanza)\n",
    "# -----------------------------\n",
    "def draw_stability(ax_box, ax_summary, df_metrics):\n",
    "    keys = [(\"precision_macro\", \"Precision (macro)\"),\n",
    "            (\"recall_macro\", \"Recall (macro)\"),\n",
    "            (\"f1_macro\", \"F1 (macro)\")]\n",
    "\n",
    "    data = []\n",
    "    ticks = []\n",
    "    lines = []\n",
    "    for k, lab in keys:\n",
    "        vals = [to_float(x) for x in df_metrics.get(k, pd.Series([])).tolist() if to_float(x) is not None]\n",
    "        data.append(vals if vals else [np.nan])\n",
    "        ticks.append(lab)\n",
    "        m, s, n = mean_std(vals, ddof=1) if vals else (None, None, 0)\n",
    "        lines.append(f\"‚Ä¢ {lab}: {fmt_mean_std(m, s, '{:.4f}')}  (n={n})\")\n",
    "\n",
    "    ax_box.set_facecolor(\"white\")\n",
    "    bp = ax_box.boxplot(data, tick_labels=ticks, patch_artist=True, widths=0.55)\n",
    "\n",
    "    box_colors = [\"#d6eaf8\", \"#fdebd0\", \"#d5f5e3\"]\n",
    "    edge_colors = [\"#5b7aa6\", \"#b9770e\", \"#1e8449\"]\n",
    "    for i, patch in enumerate(bp[\"boxes\"]):\n",
    "        patch.set(facecolor=box_colors[i], edgecolor=edge_colors[i], linewidth=1.2)\n",
    "    for med in bp[\"medians\"]:\n",
    "        med.set(color=\"#1f2d3a\", linewidth=1.4)\n",
    "\n",
    "    for i, (k, _) in enumerate(keys, start=1):\n",
    "        vals = [to_float(x) for x in df_metrics.get(k, pd.Series([])).tolist() if to_float(x) is not None]\n",
    "        if vals:\n",
    "            ax_box.scatter([i]*len(vals), vals, s=22, alpha=0.85)\n",
    "\n",
    "    ax_box.set_ylabel(\"Valore metrica\", color=\"#000000\")\n",
    "    ax_box.tick_params(colors=\"#000000\")\n",
    "    ax_box.grid(axis=\"y\", linestyle=\"--\", alpha=0.20)\n",
    "\n",
    "    ax_summary.axis(\"off\")\n",
    "    ax_summary.text(\n",
    "        0.0, 1.0,\n",
    "        \"Sintesi (media ¬± dev.std):\\n\" + \"\\n\".join(lines),\n",
    "        va=\"top\", ha=\"left\", fontsize=9, color=\"#000000\",\n",
    "        bbox=dict(facecolor=\"#fff7e6\", edgecolor=\"#f0d9a8\", boxstyle=\"round,pad=0.5\")\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "# PDF (3 pagine)\n",
    "# -----------------------------\n",
    "pdf_path = os.path.join(RESULTS_DIR, \"Report_PIPELINE_FINAL.pdf\")\n",
    "gpu_name = pick_gpu_name()\n",
    "now_str = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "with PdfPages(pdf_path) as pdf:\n",
    "\n",
    "    # =========================\n",
    "    # PAGINA 1: intro + pipeline + overall\n",
    "    # =========================\n",
    "    fig = plt.figure(figsize=(8.27, 11.69), facecolor=\"white\")\n",
    "    gs = fig.add_gridspec(nrows=24, ncols=6, left=0.06, right=0.94, top=0.97, bottom=0.06, hspace=1.10, wspace=0.8)\n",
    "\n",
    "    ax = fig.add_subplot(gs[0:2, :]); ax.axis(\"off\")\n",
    "    ax.text(0.5, 0.78, \"TEST FINALE ‚Äî Pipeline M1 (Detection) ‚Üí M2 (Classificazione)\",\n",
    "            ha=\"center\", va=\"center\", fontsize=16, fontweight=\"bold\", color=\"#000000\")\n",
    "    meta1 = f\"N test: {N_TEST if N_TEST is not None else 'N/D'} | Seed: {SEEDS_TO_TEST} | Best seed (visual): {best_seed}\"\n",
    "    meta2 = f\"{now_str} | GPU: {gpu_name} | Output: {os.path.basename(RESULTS_DIR)}\"\n",
    "    ax.text(0.5, 0.35, meta1, ha=\"center\", va=\"center\", fontsize=9, color=\"#000000\")\n",
    "    ax.text(0.5, 0.10, meta2, ha=\"center\", va=\"center\", fontsize=9, color=\"#000000\")\n",
    "\n",
    "    ax_h = fig.add_subplot(gs[2:3, :]); add_section_header(ax_h, \"1) Configurazione e logica della pipeline\", TITLE_COLOR)\n",
    "\n",
    "    ax_chk = fig.add_subplot(gs[3:6, :]); ax_chk.axis(\"off\")\n",
    "    checklist = [\n",
    "        \"‚Ä¢ M1: Detection binaria (0=Safe, 1*=Offensive dove 1*={1..4})\",\n",
    "        \"‚Ä¢ M2: Classificazione multi-classe (training solo su campioni Offensive 1..4)\",\n",
    "        \"‚Ä¢ Inference: M1 filtra; M2 gira solo sui campioni con M1=1*; merge nello spazio {0..4}\",\n",
    "        \"‚Ä¢ Output finale: etichette {0,1,2,3,4} + metriche globali/per-classe + tempi + stabilit√†\"\n",
    "    ]\n",
    "    ax_chk.text(0.0, 1.0, \"\\n\".join(checklist), va=\"top\", fontsize=9, color=\"#000000\", linespacing=1.55)\n",
    "\n",
    "    ax_flow = fig.add_subplot(gs[6:10, :])\n",
    "    draw_pipeline_diagram(ax_flow)\n",
    "\n",
    "    ax_h = fig.add_subplot(gs[10:11, :]); add_section_header(ax_h, \"2) Performance complessiva (Baseline vs Fine-Tuned)\", TITLE_COLOR)\n",
    "\n",
    "    ax_t = fig.add_subplot(gs[11:16, :])\n",
    "    t_data = [[\"Metrica\", \"Baseline\", \"Fine-Tuned (media ¬± dev.std)\"]]\n",
    "    for label, k in [(\"Accuracy\", \"accuracy\"), (\"Precision (macro)\", \"precision_macro\"), (\"Recall (macro)\", \"recall_macro\"), (\"F1 (macro)\", \"f1_macro\")]:\n",
    "        bm = fmt_val(safe_get(res_base, k, None), \"{:.4f}\")\n",
    "        m, s, _ = overall_stats.get(k, (None, None, 0))\n",
    "        ft = fmt_mean_std(m, s, \"{:.4f}\")\n",
    "        t_data.append([label, bm, ft])\n",
    "    make_table(ax_t, t_data, col_widths=[0.35, 0.25, 0.40], header_rows=1, font_size=10, row_scale=1.65)\n",
    "\n",
    "    # spazio ‚Äúrespirazione‚Äù + mini nota (non obbligatoria)\n",
    "    ax_note = fig.add_subplot(gs[16:18, :]); ax_note.axis(\"off\")\n",
    "    ax_note.text(0.0, 0.6, \"Nota: le metriche Fine-Tuned sono aggregate su seed (media ¬± dev.std).\",\n",
    "                 fontsize=9, color=\"#000000\")\n",
    "\n",
    "    pdf.savefig(fig); plt.close(fig)\n",
    "\n",
    "    # =========================\n",
    "    # PAGINA 2: stabilit√† + tempi + audit (senza overlap)\n",
    "    # =========================\n",
    "    fig = plt.figure(figsize=(8.27, 11.69), facecolor=\"white\")\n",
    "    gs = fig.add_gridspec(nrows=26, ncols=6, left=0.06, right=0.94, top=0.97, bottom=0.06, hspace=1.15, wspace=0.8)\n",
    "\n",
    "    ax = fig.add_subplot(gs[0:2, :]); ax.axis(\"off\")\n",
    "    ax.text(0.5, 0.65, \"STABILIT√Ä, TEMPI E AUDIT OPERATIVO\",\n",
    "            ha=\"center\", va=\"center\", fontsize=15, fontweight=\"bold\", color=\"#000000\")\n",
    "\n",
    "    ax_h = fig.add_subplot(gs[2:3, :]); add_section_header(ax_h, \"3) Stabilit√† tra seed (Precision/Recall/F1 macro)\", TITLE_COLOR)\n",
    "    ax_box = fig.add_subplot(gs[3:10, :])\n",
    "    ax_sum = fig.add_subplot(gs[10:13, :])   # pi√π alto -> mai sovrapposizione\n",
    "    draw_stability(ax_box, ax_sum, df)\n",
    "\n",
    "    ax_h = fig.add_subplot(gs[13:14, :]); add_section_header(ax_h, \"4) Tempi (Training e inferenza)\", TITLE_COLOR)\n",
    "    ax_tt = fig.add_subplot(gs[14:19, :])\n",
    "    t2 = [[\"Voce\", \"Valore (media ¬± dev.std)\", \"Note\"]]\n",
    "    for key, label, m, s, n, unit, fmt in time_stats:\n",
    "        note = \"N/D (non loggato)\" if m is None else f\"n={n}\"\n",
    "        t2.append([label, fmt_mean_std(m, s, fmt, unit), note])\n",
    "    make_table(ax_tt, t2, col_widths=[0.30, 0.45, 0.25], header_rows=1, font_size=9, row_scale=1.55)\n",
    "\n",
    "    ax_h = fig.add_subplot(gs[19:20, :]); add_section_header(ax_h, \"5) Audit operativo (affidabilit√† pipeline)\", TITLE_COLOR)\n",
    "    ax_at = fig.add_subplot(gs[20:26, :])\n",
    "    t3 = [[\"Indicatore\", \"Valore (media ¬± dev.std)\", \"Descrizione\"]]\n",
    "    for key, label, m, s, n, note in audit_stats:\n",
    "        if m is None:\n",
    "            val = \"N/D\"\n",
    "        else:\n",
    "            val = f\"{pct(m, 1)} ¬± {pct(s, 1) if s is not None else '0.0%'}\"\n",
    "        t3.append([label, val, fill(note, width=62)])\n",
    "    if m1_miss_m is not None:\n",
    "        t3.append([\"Missing pred M1\", f\"{pct(m1_miss_m,1)} ¬± {pct(m1_miss_s,1) if m1_miss_s is not None else '0.0%'}\",\n",
    "                   \"quota campioni senza predizione M1 (m1_missing / N)\"])\n",
    "    make_table(ax_at, t3, col_widths=[0.28, 0.22, 0.50], header_rows=1, font_size=8.2, row_scale=1.30,\n",
    "               align_left_cols=[2])\n",
    "\n",
    "    pdf.savefig(fig); plt.close(fig)\n",
    "\n",
    "    # =========================\n",
    "    # PAGINA 3: per-classe + CM + conclusioni\n",
    "    # =========================\n",
    "    fig = plt.figure(figsize=(8.27, 11.69), facecolor=\"white\")\n",
    "    gs = fig.add_gridspec(nrows=26, ncols=6, left=0.06, right=0.94, top=0.97, bottom=0.06, hspace=1.15, wspace=0.8)\n",
    "\n",
    "    ax = fig.add_subplot(gs[0:2, :]); ax.axis(\"off\")\n",
    "    ax.text(0.5, 0.65, \"DETTAGLIO PER CLASSE E MATRICE DI CONFUSIONE\",\n",
    "            ha=\"center\", va=\"center\", fontsize=15, fontweight=\"bold\", color=\"#000000\")\n",
    "    ax.text(0.5, 0.18, \"Nota: tabella = media ¬± dev.std su seed; matrice = best seed (solo visual).\",\n",
    "            ha=\"center\", va=\"center\", fontsize=9, color=\"#000000\")\n",
    "\n",
    "    ax_h = fig.add_subplot(gs[2:3, :]); add_section_header(ax_h, \"6) Metriche per classe (Baseline vs Fine-Tuned)\", TITLE_COLOR)\n",
    "    ax_pc = fig.add_subplot(gs[3:10, :])\n",
    "\n",
    "    header = [\"Classe\", \"Label\",\n",
    "              \"P (base)\", \"P (FT)\",\n",
    "              \"R (base)\", \"R (FT)\",\n",
    "              \"F1 (base)\", \"F1 (FT)\",\n",
    "              \"ŒîF1\"]\n",
    "    data = [header]\n",
    "    for c in TARGET_CLASSES_PIPELINE:\n",
    "        base_p = safe_get(res_base, f\"class_{c}_p\", None)\n",
    "        base_r = safe_get(res_base, f\"class_{c}_r\", None)\n",
    "        base_f = safe_get(res_base, f\"class_{c}_f1\", None)\n",
    "\n",
    "        ft_p_m, ft_p_s, _ = per_class_stats.get(f\"class_{c}_p\", (None, None, 0))\n",
    "        ft_r_m, ft_r_s, _ = per_class_stats.get(f\"class_{c}_r\", (None, None, 0))\n",
    "        ft_f_m, ft_f_s, _ = per_class_stats.get(f\"class_{c}_f1\", (None, None, 0))\n",
    "\n",
    "        delta_f1 = None\n",
    "        if to_float(base_f) is not None and ft_f_m is not None:\n",
    "            delta_f1 = ft_f_m - float(base_f)\n",
    "\n",
    "        data.append([\n",
    "            str(c), classes_map.get(str(c), str(c)),\n",
    "            fmt_val(base_p, \"{:.3f}\"), fmt_mean_std(ft_p_m, ft_p_s, \"{:.3f}\"),\n",
    "            fmt_val(base_r, \"{:.3f}\"), fmt_mean_std(ft_r_m, ft_r_s, \"{:.3f}\"),\n",
    "            fmt_val(base_f, \"{:.3f}\"), fmt_mean_std(ft_f_m, ft_f_s, \"{:.3f}\"),\n",
    "            fmt_val(delta_f1, \"{:+.3f}\")\n",
    "        ])\n",
    "\n",
    "    make_table(ax_pc, data,\n",
    "               col_widths=[0.06, 0.16, 0.10, 0.13, 0.10, 0.13, 0.10, 0.13, 0.09],\n",
    "               header_rows=1, font_size=9, row_scale=1.35, align_left_cols=[1])\n",
    "\n",
    "    ax_h = fig.add_subplot(gs[10:11, :]); add_section_header(ax_h, \"7) Matrice di confusione (best seed) ‚Äî conteggi reali (diagonale ‚úì)\", TITLE_COLOR)\n",
    "    ax_cm = fig.add_subplot(gs[11:20, 1:5])  # centrata\n",
    "    cm = safe_get(best_run, \"conf_matrix\", None)\n",
    "    if cm is None:\n",
    "        ax_cm.axis(\"off\")\n",
    "        ax_cm.text(0.0, 0.5, \"Nessuna confusion matrix disponibile.\", fontsize=9, color=\"#000000\")\n",
    "    else:\n",
    "        draw_easy_confusion_matrix(ax_cm, cm, row_labels=labels_short, col_labels=labels_short,\n",
    "                                   scale_x=0.70, scale_y=1.60, font_size=9)\n",
    "\n",
    "    ax_h = fig.add_subplot(gs[20:21, :]); add_section_header(ax_h, \"8) Conclusioni (bullet secchi)\", TITLE_COLOR)\n",
    "    ax_c = fig.add_subplot(gs[21:26, :]); ax_c.axis(\"off\")\n",
    "\n",
    "    act_m = next((m for k, _, m, s, n, _ in audit_stats if k == \"m2_activation_rate\"), None)\n",
    "    fb_m  = next((m for k, _, m, s, n, _ in audit_stats if k == \"reliability_risk\"), None)\n",
    "\n",
    "    bullets = [\n",
    "        \"Pipeline: M1 (0 vs 1*) ‚Üí M2 (1..4) ‚Üí merge (0..4).\",\n",
    "        f\"Fine-Tuned (media ¬± dev.std): Accuracy = {fmt_mean_std(overall_stats['accuracy'][0], overall_stats['accuracy'][1], '{:.4f}')}, \"\n",
    "        f\"F1 macro = {fmt_mean_std(overall_stats['f1_macro'][0], overall_stats['f1_macro'][1], '{:.4f}')}.\",\n",
    "        \"Per-classe: confronto baseline vs fine-tuned (P/R/F1 accoppiati) + ŒîF1.\",\n",
    "        f\"Instradamento: attivazione M2 = {pct(act_m,1) if act_m is not None else 'N/D'}; fallback/invalid su M2 = {pct(fb_m,1) if fb_m is not None else 'N/D'}.\",\n",
    "        \"Matrice: ‚úì sulla diagonale = predizioni corrette; numeri = conteggi reali.\"\n",
    "    ]\n",
    "    ax_c.text(0.0, 0.98, \"\\n\".join([f\"‚Ä¢ {b}\" for b in bullets]),\n",
    "              va=\"top\", fontsize=9, color=\"#000000\", linespacing=1.6)\n",
    "\n",
    "    pdf.savefig(fig); plt.close(fig)\n",
    "\n",
    "print(f\"‚úÖ Report PDF rigenerato (REV7): {pdf_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809a0ef0-30e6-4efe-99b8-2afa230e8b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformers (Python 3.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
