{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b0263df-a381-4cf8-acfb-cd96f7222ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pandas/core/computation/expressions.py:22: UserWarning: Pandas requires version '2.10.2' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Using MoE backend 'grouped_mm'\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "‚úÖ Ambiente Pronto e Pulito.\n",
      "   ‚Ä¢ GPU: Tesla V100S-PCIE-32GB\n",
      "   ‚Ä¢ PyTorch: 2.10.0+cu128\n",
      "   ‚Ä¢ Unsloth: 2026.2.1\n",
      "   ‚Ä¢ Transformers: 4.57.6\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELLA 0: SETUP TOTALE (MINIMAL & STABILE)\n",
    "# ==============================================================================\n",
    "import sys\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# 1. BLOCCO MODULI PROBLEMATICI\n",
    "sys.modules[\"vllm\"] = None\n",
    "sys.modules[\"vllm.sampling_params\"] = None\n",
    "\n",
    "print(\"‚è≥ Setup Ambiente in corso... (Attendere, output nascosto)\")\n",
    "\n",
    "# 2. INSTALLAZIONE & AGGIORNAMENTO SILENZIOSO\n",
    "# Scarica l'ultima versione di Unsloth da Git e aggiorna automaticamente \n",
    "# PyTorch e Transformers alle versioni pi√π recenti e compatibili.\n",
    "!pip install --upgrade --no-cache-dir --quiet \\\n",
    "    \"torch\" \"torchvision\" \"torchaudio\" \\\n",
    "    \"transformers\" \"trl\" \"peft\" \"accelerate\" \"bitsandbytes\" \\\n",
    "    \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" \\\n",
    "    \"unsloth_zoo @ git+https://github.com/unslothai/unsloth-zoo.git\" \\\n",
    "    \"pillow\" \"scikit-learn\" \"pandas\"\n",
    "\n",
    "# 3. VERIFICA E PULIZIA\n",
    "clear_output()\n",
    "\n",
    "import torch\n",
    "import unsloth\n",
    "import transformers\n",
    "from PIL import Image\n",
    "\n",
    "print(f\"‚úÖ Ambiente Pronto e Pulito.\")\n",
    "print(f\"   ‚Ä¢ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"   ‚Ä¢ PyTorch: {torch.__version__}\")\n",
    "print(f\"   ‚Ä¢ Unsloth: {unsloth.__version__}\")\n",
    "print(f\"   ‚Ä¢ Transformers: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ce37150-7cd7-474d-baff-8adcfccc8f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Caricamento Dataset HF (Eseguito una volta sola)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b73009bb27c4c61aea11e9c4b73be13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter Valid Imgs:   0%|          | 0/2373 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9699a2d02eb74aecaeedba8720b7b227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter Valid Imgs:   0%|          | 0/710 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Formattazione Dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be769ec51fb45e7b388d3eb7353d919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Formatting Train:   0%|          | 0/2373 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ce21e2855ba492ca4ee37a197d0328a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Formatting Val:   0%|          | 0/710 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset Caricato e Formattato. Train: 2373 | Val: 710\n",
      "\n",
      "üöÄ AVVIO SESSIONE DI TRAINING SU 5 SEED: [101, 285, 3692, 92, 7708]\n",
      "\n",
      "############################################################\n",
      "üé¨ RUN 1/5 | SEED CORRENTE: 101\n",
      "############################################################\n",
      "üîí Fissaggio Seed Globali a 101...\n",
      "üìÇ Cartella Output Run: outputs/Qwen2.5-VL-M1-Detection_Seed_101\n",
      "‚è≥ Inizializzazione Modello (Seed 101)...\n",
      "==((====))==  Unsloth 2026.2.1: Fast Qwen2_5_Vl patching. Transformers: 4.57.6. vLLM: 0.6.3.\n",
      "   \\\\   /|    Tesla V100S-PCIE-32GB. Num GPUs = 1. Max memory: 31.739 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 7.0. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "Unsloth: Model does not have a default image size - using 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Avvio Training Seed 101...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 2,373 | Num Epochs = 5 | Total steps = 745\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 47,589,376 of 8,339,756,032 (0.57% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='745' max='745' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [745/745 6:28:46, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.143400</td>\n",
       "      <td>0.112191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.125900</td>\n",
       "      <td>0.108429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.112500</td>\n",
       "      <td>0.107707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.110300</td>\n",
       "      <td>0.108699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.111900</td>\n",
       "      <td>0.108912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Step:   10 | Epoch: 0.07 | Loss: 4.8023\n",
      "üìù Step:   20 | Epoch: 0.13 | Loss: 4.1079\n",
      "üìù Step:   30 | Epoch: 0.20 | Loss: 2.9097\n",
      "üìù Step:   40 | Epoch: 0.27 | Loss: 2.2009\n",
      "üìù Step:   50 | Epoch: 0.34 | Loss: 1.7176\n",
      "üìù Step:   60 | Epoch: 0.40 | Loss: 1.1295\n",
      "üìù Step:   70 | Epoch: 0.47 | Loss: 0.4293\n",
      "üìù Step:   80 | Epoch: 0.54 | Loss: 0.1805\n",
      "üìù Step:   90 | Epoch: 0.61 | Loss: 0.1538\n",
      "üìù Step:  100 | Epoch: 0.67 | Loss: 0.1432\n",
      "üìù Step:  110 | Epoch: 0.74 | Loss: 0.1389\n",
      "üìù Step:  120 | Epoch: 0.81 | Loss: 0.1321\n",
      "üìù Step:  130 | Epoch: 0.88 | Loss: 0.1353\n",
      "üìù Step:  140 | Epoch: 0.94 | Loss: 0.1434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Qwen2_5_VLForConditionalGeneration does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Step:  150 | Epoch: 1.01 | Loss: 0.1333\n",
      "üìù Step:  160 | Epoch: 1.07 | Loss: 0.1339\n",
      "üìù Step:  170 | Epoch: 1.14 | Loss: 0.1339\n",
      "üìù Step:  180 | Epoch: 1.21 | Loss: 0.1474\n",
      "üìù Step:  190 | Epoch: 1.28 | Loss: 0.1229\n",
      "üìù Step:  200 | Epoch: 1.34 | Loss: 0.1357\n",
      "üìù Step:  210 | Epoch: 1.41 | Loss: 0.1352\n",
      "üìù Step:  220 | Epoch: 1.48 | Loss: 0.1269\n",
      "üìù Step:  230 | Epoch: 1.55 | Loss: 0.1285\n",
      "üìù Step:  240 | Epoch: 1.61 | Loss: 0.1212\n",
      "üìù Step:  250 | Epoch: 1.68 | Loss: 0.1377\n",
      "üìù Step:  260 | Epoch: 1.75 | Loss: 0.1235\n",
      "üìù Step:  270 | Epoch: 1.81 | Loss: 0.1235\n",
      "üìù Step:  280 | Epoch: 1.88 | Loss: 0.1293\n",
      "üìù Step:  290 | Epoch: 1.95 | Loss: 0.1259\n",
      "üìù Step:  300 | Epoch: 2.01 | Loss: 0.1278\n",
      "üìù Step:  310 | Epoch: 2.08 | Loss: 0.1176\n",
      "üìù Step:  320 | Epoch: 2.15 | Loss: 0.1217\n",
      "üìù Step:  330 | Epoch: 2.22 | Loss: 0.1364\n",
      "üìù Step:  340 | Epoch: 2.28 | Loss: 0.1232\n",
      "üìù Step:  350 | Epoch: 2.35 | Loss: 0.1132\n",
      "üìù Step:  360 | Epoch: 2.42 | Loss: 0.1214\n",
      "üìù Step:  370 | Epoch: 2.48 | Loss: 0.1254\n",
      "üìù Step:  380 | Epoch: 2.55 | Loss: 0.1203\n",
      "üìù Step:  390 | Epoch: 2.62 | Loss: 0.1120\n",
      "üìù Step:  400 | Epoch: 2.69 | Loss: 0.1203\n",
      "üìù Step:  410 | Epoch: 2.75 | Loss: 0.1319\n",
      "üìù Step:  420 | Epoch: 2.82 | Loss: 0.1159\n",
      "üìù Step:  430 | Epoch: 2.89 | Loss: 0.1238\n",
      "üìù Step:  440 | Epoch: 2.96 | Loss: 0.1125\n",
      "üìù Step:  450 | Epoch: 3.02 | Loss: 0.1304\n",
      "üìù Step:  460 | Epoch: 3.09 | Loss: 0.1153\n",
      "üìù Step:  470 | Epoch: 3.15 | Loss: 0.1184\n",
      "üìù Step:  480 | Epoch: 3.22 | Loss: 0.1128\n",
      "üìù Step:  490 | Epoch: 3.29 | Loss: 0.1203\n",
      "üìù Step:  500 | Epoch: 3.36 | Loss: 0.1158\n",
      "üìù Step:  510 | Epoch: 3.42 | Loss: 0.1061\n",
      "üìù Step:  520 | Epoch: 3.49 | Loss: 0.1162\n",
      "üìù Step:  530 | Epoch: 3.56 | Loss: 0.1152\n",
      "üìù Step:  540 | Epoch: 3.63 | Loss: 0.1095\n",
      "üìù Step:  550 | Epoch: 3.69 | Loss: 0.1168\n",
      "üìù Step:  560 | Epoch: 3.76 | Loss: 0.1027\n",
      "üìù Step:  570 | Epoch: 3.83 | Loss: 0.1109\n",
      "üìù Step:  580 | Epoch: 3.90 | Loss: 0.1249\n",
      "üìù Step:  590 | Epoch: 3.96 | Loss: 0.1103\n",
      "üìù Step:  600 | Epoch: 4.03 | Loss: 0.1121\n",
      "üìù Step:  610 | Epoch: 4.09 | Loss: 0.1027\n",
      "üìù Step:  620 | Epoch: 4.16 | Loss: 0.1167\n",
      "üìù Step:  630 | Epoch: 4.23 | Loss: 0.1089\n",
      "üìù Step:  640 | Epoch: 4.30 | Loss: 0.1118\n",
      "üìù Step:  650 | Epoch: 4.36 | Loss: 0.1115\n",
      "üìù Step:  660 | Epoch: 4.43 | Loss: 0.1079\n",
      "üìù Step:  670 | Epoch: 4.50 | Loss: 0.1094\n",
      "üìù Step:  680 | Epoch: 4.57 | Loss: 0.1088\n",
      "üìù Step:  690 | Epoch: 4.63 | Loss: 0.1111\n",
      "üìù Step:  700 | Epoch: 4.70 | Loss: 0.1142\n",
      "üìù Step:  710 | Epoch: 4.77 | Loss: 0.1096\n",
      "üìù Step:  720 | Epoch: 4.84 | Loss: 0.1058\n",
      "üìù Step:  730 | Epoch: 4.90 | Loss: 0.1121\n",
      "üìù Step:  740 | Epoch: 4.97 | Loss: 0.1119\n",
      "‚úÖ Training Finito. Durata: 389.24 min | Loss: 0.3429\n",
      "üíæ Salvataggio Artifacts...\n",
      "üì¶ Compressione ZIP in corso (attendere)...\n",
      "   -> ZIP creato: outputs/Qwen2.5-VL-M1-Detection_Seed_101_FULL_CHECKPOINTS.zip\n",
      "üßπ Pulizia VRAM per il prossimo seed...\n",
      "‚ú® Ambiente pulito.\n",
      "\n",
      "\n",
      "############################################################\n",
      "üé¨ RUN 2/5 | SEED CORRENTE: 285\n",
      "############################################################\n",
      "üîí Fissaggio Seed Globali a 285...\n",
      "üìÇ Cartella Output Run: outputs/Qwen2.5-VL-M1-Detection_Seed_285\n",
      "‚è≥ Inizializzazione Modello (Seed 285)...\n",
      "==((====))==  Unsloth 2026.2.1: Fast Qwen2_5_Vl patching. Transformers: 4.57.6. vLLM: 0.6.3.\n",
      "   \\\\   /|    Tesla V100S-PCIE-32GB. Num GPUs = 1. Max memory: 31.739 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 7.0. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "Unsloth: Model does not have a default image size - using 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Avvio Training Seed 285...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 2,373 | Num Epochs = 5 | Total steps = 745\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 47,589,376 of 8,339,756,032 (0.57% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='745' max='745' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [745/745 6:30:12, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.143400</td>\n",
       "      <td>0.112202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.126000</td>\n",
       "      <td>0.108385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.112500</td>\n",
       "      <td>0.107674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.110300</td>\n",
       "      <td>0.108579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.112000</td>\n",
       "      <td>0.108598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Step:   10 | Epoch: 0.07 | Loss: 4.8033\n",
      "üìù Step:   20 | Epoch: 0.13 | Loss: 4.1093\n",
      "üìù Step:   30 | Epoch: 0.20 | Loss: 2.9140\n",
      "üìù Step:   40 | Epoch: 0.27 | Loss: 2.2052\n",
      "üìù Step:   50 | Epoch: 0.34 | Loss: 1.7183\n",
      "üìù Step:   60 | Epoch: 0.40 | Loss: 1.1232\n",
      "üìù Step:   70 | Epoch: 0.47 | Loss: 0.4189\n",
      "üìù Step:   80 | Epoch: 0.54 | Loss: 0.1796\n",
      "üìù Step:   90 | Epoch: 0.61 | Loss: 0.1533\n",
      "üìù Step:  100 | Epoch: 0.67 | Loss: 0.1430\n",
      "üìù Step:  110 | Epoch: 0.74 | Loss: 0.1387\n",
      "üìù Step:  120 | Epoch: 0.81 | Loss: 0.1321\n",
      "üìù Step:  130 | Epoch: 0.88 | Loss: 0.1352\n",
      "üìù Step:  140 | Epoch: 0.94 | Loss: 0.1434\n",
      "üìù Step:  150 | Epoch: 1.01 | Loss: 0.1333\n",
      "üìù Step:  160 | Epoch: 1.07 | Loss: 0.1338\n",
      "üìù Step:  170 | Epoch: 1.14 | Loss: 0.1339\n",
      "üìù Step:  180 | Epoch: 1.21 | Loss: 0.1473\n",
      "üìù Step:  190 | Epoch: 1.28 | Loss: 0.1228\n",
      "üìù Step:  200 | Epoch: 1.34 | Loss: 0.1357\n",
      "üìù Step:  210 | Epoch: 1.41 | Loss: 0.1351\n",
      "üìù Step:  220 | Epoch: 1.48 | Loss: 0.1267\n",
      "üìù Step:  230 | Epoch: 1.55 | Loss: 0.1283\n",
      "üìù Step:  240 | Epoch: 1.61 | Loss: 0.1214\n",
      "üìù Step:  250 | Epoch: 1.68 | Loss: 0.1379\n",
      "üìù Step:  260 | Epoch: 1.75 | Loss: 0.1237\n",
      "üìù Step:  270 | Epoch: 1.81 | Loss: 0.1236\n",
      "üìù Step:  280 | Epoch: 1.88 | Loss: 0.1293\n",
      "üìù Step:  290 | Epoch: 1.95 | Loss: 0.1260\n",
      "üìù Step:  300 | Epoch: 2.01 | Loss: 0.1277\n",
      "üìù Step:  310 | Epoch: 2.08 | Loss: 0.1176\n",
      "üìù Step:  320 | Epoch: 2.15 | Loss: 0.1216\n",
      "üìù Step:  330 | Epoch: 2.22 | Loss: 0.1365\n",
      "üìù Step:  340 | Epoch: 2.28 | Loss: 0.1233\n",
      "üìù Step:  350 | Epoch: 2.35 | Loss: 0.1131\n",
      "üìù Step:  360 | Epoch: 2.42 | Loss: 0.1214\n",
      "üìù Step:  370 | Epoch: 2.48 | Loss: 0.1254\n",
      "üìù Step:  380 | Epoch: 2.55 | Loss: 0.1204\n",
      "üìù Step:  390 | Epoch: 2.62 | Loss: 0.1122\n",
      "üìù Step:  400 | Epoch: 2.69 | Loss: 0.1203\n",
      "üìù Step:  410 | Epoch: 2.75 | Loss: 0.1319\n",
      "üìù Step:  420 | Epoch: 2.82 | Loss: 0.1158\n",
      "üìù Step:  430 | Epoch: 2.89 | Loss: 0.1239\n",
      "üìù Step:  440 | Epoch: 2.96 | Loss: 0.1125\n",
      "üìù Step:  450 | Epoch: 3.02 | Loss: 0.1305\n",
      "üìù Step:  460 | Epoch: 3.09 | Loss: 0.1154\n",
      "üìù Step:  470 | Epoch: 3.15 | Loss: 0.1185\n",
      "üìù Step:  480 | Epoch: 3.22 | Loss: 0.1127\n",
      "üìù Step:  490 | Epoch: 3.29 | Loss: 0.1204\n",
      "üìù Step:  500 | Epoch: 3.36 | Loss: 0.1159\n",
      "üìù Step:  510 | Epoch: 3.42 | Loss: 0.1063\n",
      "üìù Step:  520 | Epoch: 3.49 | Loss: 0.1163\n",
      "üìù Step:  530 | Epoch: 3.56 | Loss: 0.1152\n",
      "üìù Step:  540 | Epoch: 3.63 | Loss: 0.1094\n",
      "üìù Step:  550 | Epoch: 3.69 | Loss: 0.1168\n",
      "üìù Step:  560 | Epoch: 3.76 | Loss: 0.1025\n",
      "üìù Step:  570 | Epoch: 3.83 | Loss: 0.1108\n",
      "üìù Step:  580 | Epoch: 3.90 | Loss: 0.1248\n",
      "üìù Step:  590 | Epoch: 3.96 | Loss: 0.1103\n",
      "üìù Step:  600 | Epoch: 4.03 | Loss: 0.1123\n",
      "üìù Step:  610 | Epoch: 4.09 | Loss: 0.1029\n",
      "üìù Step:  620 | Epoch: 4.16 | Loss: 0.1167\n",
      "üìù Step:  630 | Epoch: 4.23 | Loss: 0.1088\n",
      "üìù Step:  640 | Epoch: 4.30 | Loss: 0.1118\n",
      "üìù Step:  650 | Epoch: 4.36 | Loss: 0.1114\n",
      "üìù Step:  660 | Epoch: 4.43 | Loss: 0.1080\n",
      "üìù Step:  670 | Epoch: 4.50 | Loss: 0.1094\n",
      "üìù Step:  680 | Epoch: 4.57 | Loss: 0.1089\n",
      "üìù Step:  690 | Epoch: 4.63 | Loss: 0.1113\n",
      "üìù Step:  700 | Epoch: 4.70 | Loss: 0.1141\n",
      "üìù Step:  710 | Epoch: 4.77 | Loss: 0.1095\n",
      "üìù Step:  720 | Epoch: 4.84 | Loss: 0.1057\n",
      "üìù Step:  730 | Epoch: 4.90 | Loss: 0.1119\n",
      "üìù Step:  740 | Epoch: 4.97 | Loss: 0.1120\n",
      "‚úÖ Training Finito. Durata: 390.45 min | Loss: 0.3428\n",
      "üíæ Salvataggio Artifacts...\n",
      "üì¶ Compressione ZIP in corso (attendere)...\n",
      "   -> ZIP creato: outputs/Qwen2.5-VL-M1-Detection_Seed_285_FULL_CHECKPOINTS.zip\n",
      "üßπ Pulizia VRAM per il prossimo seed...\n",
      "‚ú® Ambiente pulito.\n",
      "\n",
      "\n",
      "############################################################\n",
      "üé¨ RUN 3/5 | SEED CORRENTE: 3692\n",
      "############################################################\n",
      "üîí Fissaggio Seed Globali a 3692...\n",
      "üìÇ Cartella Output Run: outputs/Qwen2.5-VL-M1-Detection_Seed_3692\n",
      "‚è≥ Inizializzazione Modello (Seed 3692)...\n",
      "==((====))==  Unsloth 2026.2.1: Fast Qwen2_5_Vl patching. Transformers: 4.57.6. vLLM: 0.6.3.\n",
      "   \\\\   /|    Tesla V100S-PCIE-32GB. Num GPUs = 1. Max memory: 31.739 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 7.0. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "Unsloth: Model does not have a default image size - using 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Avvio Training Seed 3692...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 2,373 | Num Epochs = 5 | Total steps = 745\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 47,589,376 of 8,339,756,032 (0.57% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='745' max='745' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [745/745 6:31:35, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.143300</td>\n",
       "      <td>0.112202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.125900</td>\n",
       "      <td>0.108330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.112400</td>\n",
       "      <td>0.107865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.110300</td>\n",
       "      <td>0.108784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.111900</td>\n",
       "      <td>0.108924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Step:   10 | Epoch: 0.07 | Loss: 4.8040\n",
      "üìù Step:   20 | Epoch: 0.13 | Loss: 4.1052\n",
      "üìù Step:   30 | Epoch: 0.20 | Loss: 2.9080\n",
      "üìù Step:   40 | Epoch: 0.27 | Loss: 2.2000\n",
      "üìù Step:   50 | Epoch: 0.34 | Loss: 1.7154\n",
      "üìù Step:   60 | Epoch: 0.40 | Loss: 1.1235\n",
      "üìù Step:   70 | Epoch: 0.47 | Loss: 0.4194\n",
      "üìù Step:   80 | Epoch: 0.54 | Loss: 0.1791\n",
      "üìù Step:   90 | Epoch: 0.61 | Loss: 0.1526\n",
      "üìù Step:  100 | Epoch: 0.67 | Loss: 0.1430\n",
      "üìù Step:  110 | Epoch: 0.74 | Loss: 0.1387\n",
      "üìù Step:  120 | Epoch: 0.81 | Loss: 0.1321\n",
      "üìù Step:  130 | Epoch: 0.88 | Loss: 0.1352\n",
      "üìù Step:  140 | Epoch: 0.94 | Loss: 0.1433\n",
      "üìù Step:  150 | Epoch: 1.01 | Loss: 0.1332\n",
      "üìù Step:  160 | Epoch: 1.07 | Loss: 0.1338\n",
      "üìù Step:  170 | Epoch: 1.14 | Loss: 0.1339\n",
      "üìù Step:  180 | Epoch: 1.21 | Loss: 0.1472\n",
      "üìù Step:  190 | Epoch: 1.28 | Loss: 0.1228\n",
      "üìù Step:  200 | Epoch: 1.34 | Loss: 0.1357\n",
      "üìù Step:  210 | Epoch: 1.41 | Loss: 0.1352\n",
      "üìù Step:  220 | Epoch: 1.48 | Loss: 0.1267\n",
      "üìù Step:  230 | Epoch: 1.55 | Loss: 0.1283\n",
      "üìù Step:  240 | Epoch: 1.61 | Loss: 0.1211\n",
      "üìù Step:  250 | Epoch: 1.68 | Loss: 0.1378\n",
      "üìù Step:  260 | Epoch: 1.75 | Loss: 0.1237\n",
      "üìù Step:  270 | Epoch: 1.81 | Loss: 0.1233\n",
      "üìù Step:  280 | Epoch: 1.88 | Loss: 0.1294\n",
      "üìù Step:  290 | Epoch: 1.95 | Loss: 0.1259\n",
      "üìù Step:  300 | Epoch: 2.01 | Loss: 0.1277\n",
      "üìù Step:  310 | Epoch: 2.08 | Loss: 0.1176\n",
      "üìù Step:  320 | Epoch: 2.15 | Loss: 0.1216\n",
      "üìù Step:  330 | Epoch: 2.22 | Loss: 0.1363\n",
      "üìù Step:  340 | Epoch: 2.28 | Loss: 0.1231\n",
      "üìù Step:  350 | Epoch: 2.35 | Loss: 0.1128\n",
      "üìù Step:  360 | Epoch: 2.42 | Loss: 0.1210\n",
      "üìù Step:  370 | Epoch: 2.48 | Loss: 0.1254\n",
      "üìù Step:  380 | Epoch: 2.55 | Loss: 0.1201\n",
      "üìù Step:  390 | Epoch: 2.62 | Loss: 0.1122\n",
      "üìù Step:  400 | Epoch: 2.69 | Loss: 0.1205\n",
      "üìù Step:  410 | Epoch: 2.75 | Loss: 0.1319\n",
      "üìù Step:  420 | Epoch: 2.82 | Loss: 0.1160\n",
      "üìù Step:  430 | Epoch: 2.89 | Loss: 0.1238\n",
      "üìù Step:  440 | Epoch: 2.96 | Loss: 0.1124\n",
      "üìù Step:  450 | Epoch: 3.02 | Loss: 0.1304\n",
      "üìù Step:  460 | Epoch: 3.09 | Loss: 0.1153\n",
      "üìù Step:  470 | Epoch: 3.15 | Loss: 0.1182\n",
      "üìù Step:  480 | Epoch: 3.22 | Loss: 0.1129\n",
      "üìù Step:  490 | Epoch: 3.29 | Loss: 0.1203\n",
      "üìù Step:  500 | Epoch: 3.36 | Loss: 0.1158\n",
      "üìù Step:  510 | Epoch: 3.42 | Loss: 0.1062\n",
      "üìù Step:  520 | Epoch: 3.49 | Loss: 0.1161\n",
      "üìù Step:  530 | Epoch: 3.56 | Loss: 0.1152\n",
      "üìù Step:  540 | Epoch: 3.63 | Loss: 0.1093\n",
      "üìù Step:  550 | Epoch: 3.69 | Loss: 0.1167\n",
      "üìù Step:  560 | Epoch: 3.76 | Loss: 0.1026\n",
      "üìù Step:  570 | Epoch: 3.83 | Loss: 0.1107\n",
      "üìù Step:  580 | Epoch: 3.90 | Loss: 0.1249\n",
      "üìù Step:  590 | Epoch: 3.96 | Loss: 0.1103\n",
      "üìù Step:  600 | Epoch: 4.03 | Loss: 0.1120\n",
      "üìù Step:  610 | Epoch: 4.09 | Loss: 0.1025\n",
      "üìù Step:  620 | Epoch: 4.16 | Loss: 0.1166\n",
      "üìù Step:  630 | Epoch: 4.23 | Loss: 0.1090\n",
      "üìù Step:  640 | Epoch: 4.30 | Loss: 0.1118\n",
      "üìù Step:  650 | Epoch: 4.36 | Loss: 0.1114\n",
      "üìù Step:  660 | Epoch: 4.43 | Loss: 0.1078\n",
      "üìù Step:  670 | Epoch: 4.50 | Loss: 0.1093\n",
      "üìù Step:  680 | Epoch: 4.57 | Loss: 0.1088\n",
      "üìù Step:  690 | Epoch: 4.63 | Loss: 0.1111\n",
      "üìù Step:  700 | Epoch: 4.70 | Loss: 0.1141\n",
      "üìù Step:  710 | Epoch: 4.77 | Loss: 0.1094\n",
      "üìù Step:  720 | Epoch: 4.84 | Loss: 0.1057\n",
      "üìù Step:  730 | Epoch: 4.90 | Loss: 0.1117\n",
      "üìù Step:  740 | Epoch: 4.97 | Loss: 0.1119\n",
      "‚úÖ Training Finito. Durata: 391.83 min | Loss: 0.3425\n",
      "üíæ Salvataggio Artifacts...\n",
      "üì¶ Compressione ZIP in corso (attendere)...\n",
      "   -> ZIP creato: outputs/Qwen2.5-VL-M1-Detection_Seed_3692_FULL_CHECKPOINTS.zip\n",
      "üßπ Pulizia VRAM per il prossimo seed...\n",
      "‚ú® Ambiente pulito.\n",
      "\n",
      "\n",
      "############################################################\n",
      "üé¨ RUN 4/5 | SEED CORRENTE: 92\n",
      "############################################################\n",
      "üîí Fissaggio Seed Globali a 92...\n",
      "üìÇ Cartella Output Run: outputs/Qwen2.5-VL-M1-Detection_Seed_92\n",
      "‚è≥ Inizializzazione Modello (Seed 92)...\n",
      "==((====))==  Unsloth 2026.2.1: Fast Qwen2_5_Vl patching. Transformers: 4.57.6. vLLM: 0.6.3.\n",
      "   \\\\   /|    Tesla V100S-PCIE-32GB. Num GPUs = 1. Max memory: 31.739 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 7.0. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "Unsloth: Model does not have a default image size - using 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Avvio Training Seed 92...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 2,373 | Num Epochs = 5 | Total steps = 745\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 47,589,376 of 8,339,756,032 (0.57% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='745' max='745' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [745/745 6:34:16, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.143300</td>\n",
       "      <td>0.112126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.126000</td>\n",
       "      <td>0.108541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.112400</td>\n",
       "      <td>0.107768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.110300</td>\n",
       "      <td>0.108632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.111800</td>\n",
       "      <td>0.108652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Step:   10 | Epoch: 0.07 | Loss: 4.8032\n",
      "üìù Step:   20 | Epoch: 0.13 | Loss: 4.1107\n",
      "üìù Step:   30 | Epoch: 0.20 | Loss: 2.9113\n",
      "üìù Step:   40 | Epoch: 0.27 | Loss: 2.2013\n",
      "üìù Step:   50 | Epoch: 0.34 | Loss: 1.7149\n",
      "üìù Step:   60 | Epoch: 0.40 | Loss: 1.1215\n",
      "üìù Step:   70 | Epoch: 0.47 | Loss: 0.4210\n",
      "üìù Step:   80 | Epoch: 0.54 | Loss: 0.1797\n",
      "üìù Step:   90 | Epoch: 0.61 | Loss: 0.1534\n",
      "üìù Step:  100 | Epoch: 0.67 | Loss: 0.1431\n",
      "üìù Step:  110 | Epoch: 0.74 | Loss: 0.1388\n",
      "üìù Step:  120 | Epoch: 0.81 | Loss: 0.1320\n",
      "üìù Step:  130 | Epoch: 0.88 | Loss: 0.1352\n",
      "üìù Step:  140 | Epoch: 0.94 | Loss: 0.1433\n",
      "üìù Step:  150 | Epoch: 1.01 | Loss: 0.1332\n",
      "üìù Step:  160 | Epoch: 1.07 | Loss: 0.1339\n",
      "üìù Step:  170 | Epoch: 1.14 | Loss: 0.1339\n",
      "üìù Step:  180 | Epoch: 1.21 | Loss: 0.1473\n",
      "üìù Step:  190 | Epoch: 1.28 | Loss: 0.1228\n",
      "üìù Step:  200 | Epoch: 1.34 | Loss: 0.1356\n",
      "üìù Step:  210 | Epoch: 1.41 | Loss: 0.1352\n",
      "üìù Step:  220 | Epoch: 1.48 | Loss: 0.1267\n",
      "üìù Step:  230 | Epoch: 1.55 | Loss: 0.1283\n",
      "üìù Step:  240 | Epoch: 1.61 | Loss: 0.1209\n",
      "üìù Step:  250 | Epoch: 1.68 | Loss: 0.1376\n",
      "üìù Step:  260 | Epoch: 1.75 | Loss: 0.1235\n",
      "üìù Step:  270 | Epoch: 1.81 | Loss: 0.1233\n",
      "üìù Step:  280 | Epoch: 1.88 | Loss: 0.1293\n",
      "üìù Step:  290 | Epoch: 1.95 | Loss: 0.1260\n",
      "üìù Step:  300 | Epoch: 2.01 | Loss: 0.1279\n",
      "üìù Step:  310 | Epoch: 2.08 | Loss: 0.1176\n",
      "üìù Step:  320 | Epoch: 2.15 | Loss: 0.1217\n",
      "üìù Step:  330 | Epoch: 2.22 | Loss: 0.1362\n",
      "üìù Step:  340 | Epoch: 2.28 | Loss: 0.1230\n",
      "üìù Step:  350 | Epoch: 2.35 | Loss: 0.1130\n",
      "üìù Step:  360 | Epoch: 2.42 | Loss: 0.1211\n",
      "üìù Step:  370 | Epoch: 2.48 | Loss: 0.1253\n",
      "üìù Step:  380 | Epoch: 2.55 | Loss: 0.1203\n",
      "üìù Step:  390 | Epoch: 2.62 | Loss: 0.1120\n",
      "üìù Step:  400 | Epoch: 2.69 | Loss: 0.1203\n",
      "üìù Step:  410 | Epoch: 2.75 | Loss: 0.1319\n",
      "üìù Step:  420 | Epoch: 2.82 | Loss: 0.1157\n",
      "üìù Step:  430 | Epoch: 2.89 | Loss: 0.1236\n",
      "üìù Step:  440 | Epoch: 2.96 | Loss: 0.1124\n",
      "üìù Step:  450 | Epoch: 3.02 | Loss: 0.1304\n",
      "üìù Step:  460 | Epoch: 3.09 | Loss: 0.1151\n",
      "üìù Step:  470 | Epoch: 3.15 | Loss: 0.1184\n",
      "üìù Step:  480 | Epoch: 3.22 | Loss: 0.1127\n",
      "üìù Step:  490 | Epoch: 3.29 | Loss: 0.1203\n",
      "üìù Step:  500 | Epoch: 3.36 | Loss: 0.1157\n",
      "üìù Step:  510 | Epoch: 3.42 | Loss: 0.1061\n",
      "üìù Step:  520 | Epoch: 3.49 | Loss: 0.1161\n",
      "üìù Step:  530 | Epoch: 3.56 | Loss: 0.1152\n",
      "üìù Step:  540 | Epoch: 3.63 | Loss: 0.1093\n",
      "üìù Step:  550 | Epoch: 3.69 | Loss: 0.1168\n",
      "üìù Step:  560 | Epoch: 3.76 | Loss: 0.1025\n",
      "üìù Step:  570 | Epoch: 3.83 | Loss: 0.1106\n",
      "üìù Step:  580 | Epoch: 3.90 | Loss: 0.1247\n",
      "üìù Step:  590 | Epoch: 3.96 | Loss: 0.1103\n",
      "üìù Step:  600 | Epoch: 4.03 | Loss: 0.1119\n",
      "üìù Step:  610 | Epoch: 4.09 | Loss: 0.1025\n",
      "üìù Step:  620 | Epoch: 4.16 | Loss: 0.1164\n",
      "üìù Step:  630 | Epoch: 4.23 | Loss: 0.1088\n",
      "üìù Step:  640 | Epoch: 4.30 | Loss: 0.1116\n",
      "üìù Step:  650 | Epoch: 4.36 | Loss: 0.1113\n",
      "üìù Step:  660 | Epoch: 4.43 | Loss: 0.1079\n",
      "üìù Step:  670 | Epoch: 4.50 | Loss: 0.1093\n",
      "üìù Step:  680 | Epoch: 4.57 | Loss: 0.1088\n",
      "üìù Step:  690 | Epoch: 4.63 | Loss: 0.1110\n",
      "üìù Step:  700 | Epoch: 4.70 | Loss: 0.1140\n",
      "üìù Step:  710 | Epoch: 4.77 | Loss: 0.1095\n",
      "üìù Step:  720 | Epoch: 4.84 | Loss: 0.1056\n",
      "üìù Step:  730 | Epoch: 4.90 | Loss: 0.1117\n",
      "üìù Step:  740 | Epoch: 4.97 | Loss: 0.1118\n",
      "‚úÖ Training Finito. Durata: 394.53 min | Loss: 0.3426\n",
      "üíæ Salvataggio Artifacts...\n",
      "üì¶ Compressione ZIP in corso (attendere)...\n",
      "   -> ZIP creato: outputs/Qwen2.5-VL-M1-Detection_Seed_92_FULL_CHECKPOINTS.zip\n",
      "üßπ Pulizia VRAM per il prossimo seed...\n",
      "‚ú® Ambiente pulito.\n",
      "\n",
      "\n",
      "############################################################\n",
      "üé¨ RUN 5/5 | SEED CORRENTE: 7708\n",
      "############################################################\n",
      "üîí Fissaggio Seed Globali a 7708...\n",
      "üìÇ Cartella Output Run: outputs/Qwen2.5-VL-M1-Detection_Seed_7708\n",
      "‚è≥ Inizializzazione Modello (Seed 7708)...\n",
      "==((====))==  Unsloth 2026.2.1: Fast Qwen2_5_Vl patching. Transformers: 4.57.6. vLLM: 0.6.3.\n",
      "   \\\\   /|    Tesla V100S-PCIE-32GB. Num GPUs = 1. Max memory: 31.739 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 7.0. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "Unsloth: Model does not have a default image size - using 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Avvio Training Seed 7708...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 2,373 | Num Epochs = 5 | Total steps = 745\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 47,589,376 of 8,339,756,032 (0.57% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='117' max='745' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [117/745 54:13 < 4:56:07, 0.04 it/s, Epoch 0.78/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Step:   10 | Epoch: 0.07 | Loss: 4.8030\n",
      "üìù Step:   20 | Epoch: 0.13 | Loss: 4.1080\n",
      "üìù Step:   30 | Epoch: 0.20 | Loss: 2.9080\n",
      "üìù Step:   40 | Epoch: 0.27 | Loss: 2.1999\n",
      "üìù Step:   50 | Epoch: 0.34 | Loss: 1.7117\n",
      "üìù Step:   60 | Epoch: 0.40 | Loss: 1.1156\n",
      "üìù Step:   70 | Epoch: 0.47 | Loss: 0.4184\n",
      "üìù Step:   80 | Epoch: 0.54 | Loss: 0.1795\n",
      "üìù Step:   90 | Epoch: 0.61 | Loss: 0.1531\n",
      "üìù Step:  100 | Epoch: 0.67 | Loss: 0.1431\n",
      "üìù Step:  110 | Epoch: 0.74 | Loss: 0.1389\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 199\u001b[0m\n\u001b[1;32m    196\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m    198\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 199\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    202\u001b[0m total_duration \u001b[38;5;241m=\u001b[39m (end_time \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m60\u001b[39m\n",
      "File \u001b[0;32m~/unsloth_compiled_cache/UnslothSFTTrainer.py:65\u001b[0m, in \u001b[0;36mprepare_for_training_mode.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor_training\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfor_training(use_gradient_checkpointing\u001b[38;5;241m=\u001b[39muse_gc)\n\u001b[0;32m---> 65\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Restore previous mode when possible\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor_inference\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/trainer.py:2325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:328\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "File \u001b[0;32m~/unsloth_compiled_cache/UnslothSFTTrainer.py:1233\u001b[0m, in \u001b[0;36m_UnslothSFTTrainer.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1232\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaybe_activation_offload_context:\n\u001b[0;32m-> 1233\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:91\u001b[0m, in \u001b[0;36m_unsloth_training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/accelerate/accelerator.py:2848\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2846\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2847\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2848\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2849\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[1;32m   2850\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/_tensor.py:630\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    622\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    623\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    628\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    629\u001b[0m     )\n\u001b[0;32m--> 630\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/autograd/__init__.py:364\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    359\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    361\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/autograd/graph.py:865\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    863\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    864\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 865\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import shutil\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import unsloth\n",
    "from datetime import datetime\n",
    "from datasets import load_from_disk\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from unsloth import FastVisionModel, UnslothVisionDataCollator, is_bfloat16_supported\n",
    "from transformers import TrainerCallback, set_seed\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURAZIONE GLOBALE (Fissa per tutte le run)\n",
    "# ==============================================================================\n",
    "SEEDS = [101, 285, 3692, 92]  # <--- LISTA DI SEED DA TESTARE\n",
    "NUM_EPOCHS = 5                # <--- NUMERO DI EPOCHE PER OGNI RUN (con SEED X)\n",
    "MODEL_ID = \"unsloth/Qwen2.5-VL-7B-Instruct-bnb-4bit\"\n",
    "MODEL_SHORTNAME = \"Qwen2.5-VL-M1-Detection\"\n",
    "DATASET_PATH = os.path.join(\"DATASET_ITA\", \"PROCESSED_DATA\", \"HF_DATASETS\", \"M1_detection\")\n",
    "\n",
    "# SYSTEM PROMPT\n",
    "SYSTEM_INSTRUCTION_M1 = \"\"\"Sei un classificatore binario esperto per la moderazione di contenuti social.\n",
    "Valuta CONGIUNTAMENTE il commento testuale e i frame del video associato.\n",
    "\n",
    "Il tuo compito √® stabilire OGGETTIVAMENTE se √® presente contenuto offensivo riconducibile ad una di queste categorie:\n",
    "- Flaming: insulti, linguaggio aggressivo, minacce, parolacce dirette a una persona o gruppo.\n",
    "- Denigration: umiliazione, disprezzo, svalutazione o ridicolizzazione di una persona o gruppo.\n",
    "- Sexual: molestie, allusioni o offese a sfondo sessuale, oggettivazione o inviti espliciti.\n",
    "- Racism: stereotipi, offese o discriminazione verso etnia, razza, cultura o nazionalit√†.\n",
    "\n",
    "Regola decisionale:\n",
    "- Rispondi 1 se rilevi contenuto offensivo riconducibile alle categorie sopra indicate.\n",
    "- Rispondi 0 se il contenuto √® neutro, positivo, o se si tratta di critica costruttiva/ironia non offensiva.\n",
    "\n",
    "Linee guida:\n",
    "- Basati solo sull'evidenza presente nei dati (testo + immagini).\n",
    "- Non essere n√© troppo severo n√© troppo permissivo: attieniti alle definizioni.\n",
    "\n",
    "Formato di output (OBBLIGATORIO):\n",
    "Rispondi esclusivamente con un singolo carattere: 1 oppure 0.\n",
    "Non aggiungere spiegazioni, punteggiatura o altro testo.\"\"\"\n",
    "\n",
    "# Callback per monitoraggio\n",
    "class RealTimePrinterCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and \"loss\" in logs:\n",
    "            print(f\"üìù Step: {state.global_step:4d} | Epoch: {logs['epoch']:.2f} | Loss: {logs['loss']:.4f}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CARICAMENTO E FORMATTAZIONE DATASET (Una volta sola per efficienza)\n",
    "# ==============================================================================\n",
    "print(\"üìÇ Caricamento Dataset HF (Eseguito una volta sola)...\")\n",
    "dataset_raw = load_from_disk(DATASET_PATH)\n",
    "\n",
    "def has_valid_images(sample):\n",
    "    user_msg = sample[\"messages\"][0]\n",
    "    for item in user_msg[\"content\"]:\n",
    "        if item[\"type\"] == \"image\":\n",
    "            raw_path = item[\"image\"]\n",
    "            clean_path = raw_path.replace(\"file://\", \"\")\n",
    "            check_path = \"/\" + clean_path.lstrip(\"/\") if clean_path else \"\"\n",
    "            if not os.path.exists(check_path):\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "# Filtriamo eventuali immagini rotte\n",
    "train_valid = dataset_raw[\"train\"].filter(has_valid_images, desc=\"Filter Valid Imgs\")\n",
    "val_valid = dataset_raw[\"val\"].filter(has_valid_images, desc=\"Filter Valid Imgs\")\n",
    "\n",
    "def format_multimodal_sample(sample):\n",
    "    raw_user_msg = sample[\"messages\"][0]\n",
    "    raw_assistant_msg = sample[\"messages\"][1] \n",
    "    user_content = []\n",
    "    \n",
    "    for item in raw_user_msg[\"content\"]:\n",
    "        if item[\"type\"] == \"image\":\n",
    "            raw_path = item[\"image\"]\n",
    "            clean_path = raw_path.replace(\"file://\", \"\")\n",
    "            clean_path = \"/\" + clean_path.lstrip(\"/\") \n",
    "            final_path = f\"file://{clean_path}\"\n",
    "            user_content.append({\"type\": \"image\", \"image\": final_path})\n",
    "        elif item[\"type\"] == \"text\":\n",
    "            text_clean = item[\"text\"].replace(\"Commento:\", \"\").strip().strip('\"').strip(\"'\")\n",
    "            text_final = f\"Commento: \\\"{text_clean}\\\"\"\n",
    "            user_content.append({\"type\": \"text\", \"text\": text_final})\n",
    "            \n",
    "    label_text = raw_assistant_msg[\"content\"][0][\"text\"]\n",
    "\n",
    "    new_messages = [\n",
    "        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": SYSTEM_INSTRUCTION_M1}]},\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "        {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": label_text}]}\n",
    "    ]\n",
    "    return {\"messages\": new_messages}\n",
    "\n",
    "print(\"üîÑ Formattazione Dataset...\")\n",
    "train_dataset = train_valid.map(format_multimodal_sample, batched=False, desc=\"Formatting Train\")\n",
    "val_dataset = val_valid.map(format_multimodal_sample, batched=False, desc=\"Formatting Val\")\n",
    "print(f\"‚úÖ Dataset Caricato e Formattato. Train: {len(train_dataset)} | Val: {len(val_dataset)}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. MEGA-LOOP DI TRAINING (TUTTI I SEEDS)\n",
    "# ==============================================================================\n",
    "print(f\"\\nüöÄ AVVIO SESSIONE DI TRAINING SU {len(SEEDS)} SEED: {SEEDS}\")\n",
    "\n",
    "for seed_idx, TRAINING_SEED in enumerate(SEEDS):\n",
    "    print(\"\\n\" + \"#\"*60)\n",
    "    print(f\"üé¨ RUN {seed_idx + 1}/{len(SEEDS)} | SEED CORRENTE: {TRAINING_SEED}\")\n",
    "    print(\"#\"*60)\n",
    "\n",
    "    # --- FIX DETERMINISMO GLOBALE ---\n",
    "    print(f\"üîí Fissaggio Seed Globali a {TRAINING_SEED}...\")\n",
    "    random.seed(TRAINING_SEED)\n",
    "    np.random.seed(TRAINING_SEED)\n",
    "    torch.manual_seed(TRAINING_SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(TRAINING_SEED)\n",
    "    set_seed(TRAINING_SEED)\n",
    "    # -------------------------------------------------------\n",
    "\n",
    "    # Definizione Output Directory Dinamica\n",
    "    OUTPUT_DIR = f\"outputs/{MODEL_SHORTNAME}_Seed_{TRAINING_SEED}\"\n",
    "    print(f\"üìÇ Cartella Output Run: {OUTPUT_DIR}\")\n",
    "\n",
    "    # --- A. CARICAMENTO MODELLO ---\n",
    "    print(f\"‚è≥ Inizializzazione Modello (Seed {TRAINING_SEED})...\")\n",
    "    model, tokenizer = FastVisionModel.from_pretrained(\n",
    "        model_name = MODEL_ID,\n",
    "        load_in_4bit = True,\n",
    "        use_gradient_checkpointing = \"unsloth\",\n",
    "    )\n",
    "\n",
    "    model = FastVisionModel.get_peft_model(\n",
    "        model,\n",
    "        r = 16,\n",
    "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        lora_alpha = 16,\n",
    "        lora_dropout = 0,\n",
    "        bias = \"none\",\n",
    "        random_state = TRAINING_SEED,\n",
    "        use_rslora = False,\n",
    "        loftq_config = None,\n",
    "    )\n",
    "    FastVisionModel.for_training(model)\n",
    "\n",
    "    # --- B. CONFIGURAZIONE TRAINER ---\n",
    "    training_args = SFTConfig(\n",
    "        per_device_train_batch_size = 4,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        num_train_epochs = NUM_EPOCHS,\n",
    "        learning_rate = 5e-5,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        warmup_ratio = 0.1,\n",
    "        weight_decay = 0.01,\n",
    "        optim = \"adamw_8bit\",\n",
    "        max_grad_norm = 0.3,\n",
    "        \n",
    "        # Salvataggio\n",
    "        eval_strategy = \"epoch\",\n",
    "        save_strategy = \"epoch\",\n",
    "        save_total_limit = None,\n",
    "        load_best_model_at_end = False,\n",
    "        metric_for_best_model = \"eval_loss\",\n",
    "        greater_is_better = False,\n",
    "        \n",
    "        # Hardware & Path\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        gradient_checkpointing = True,\n",
    "        logging_steps = 10,\n",
    "        output_dir = OUTPUT_DIR,\n",
    "        report_to = \"none\",\n",
    "        \n",
    "        # Unsloth\n",
    "        remove_unused_columns = False,\n",
    "        dataset_text_field = \"\",\n",
    "        dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
    "        seed = TRAINING_SEED,\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model = model,\n",
    "        tokenizer = tokenizer,\n",
    "        data_collator = UnslothVisionDataCollator(model, tokenizer),\n",
    "        train_dataset = train_dataset,\n",
    "        eval_dataset = val_dataset,\n",
    "        args = training_args,\n",
    "        callbacks = [RealTimePrinterCallback()],\n",
    "    )\n",
    "\n",
    "    # --- C. ESECUZIONE TRAINING ---\n",
    "    print(f\"üî• Avvio Training Seed {TRAINING_SEED}...\")\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    trainer_stats = trainer.train()\n",
    "    end_time = time.time()\n",
    "    \n",
    "    total_duration = (end_time - start_time) / 60\n",
    "    final_train_loss = trainer_stats.training_loss\n",
    "    global_steps_done = trainer_stats.global_step\n",
    "\n",
    "    print(f\"‚úÖ Training Finito. Durata: {total_duration:.2f} min | Loss: {final_train_loss:.4f}\")\n",
    "\n",
    "    # --- D. SALVATAGGIO ---\n",
    "    ADAPTER_PATH = os.path.join(OUTPUT_DIR, \"final_adapter_latest\")\n",
    "    REPORT_FILENAME = f\"training_report_Seed_{TRAINING_SEED}.json\"\n",
    "    REPORT_PATH = os.path.join(OUTPUT_DIR, REPORT_FILENAME)\n",
    "    ZIP_FILENAME = f\"{MODEL_SHORTNAME}_Seed_{TRAINING_SEED}_FULL_CHECKPOINTS\"\n",
    "    \n",
    "    # Cartella Padre per lo ZIP (Per evitare Inception bug)\n",
    "    PARENT_DIR = os.path.dirname(OUTPUT_DIR)\n",
    "    ZIP_FULL_PATH = os.path.join(PARENT_DIR, ZIP_FILENAME)\n",
    "\n",
    "    os.makedirs(ADAPTER_PATH, exist_ok=True)\n",
    "    \n",
    "    print(f\"üíæ Salvataggio Artifacts...\")\n",
    "    model.save_pretrained(ADAPTER_PATH)\n",
    "    tokenizer.save_pretrained(ADAPTER_PATH)\n",
    "\n",
    "    # Report JSON\n",
    "    peft_config_data = \"N/A\"\n",
    "    try:\n",
    "        raw_config = getattr(model, \"peft_config\", None)\n",
    "        if isinstance(raw_config, dict) and raw_config.get(\"default\"):\n",
    "            peft_config_data = str(raw_config[\"default\"])\n",
    "    except: pass\n",
    "\n",
    "    full_report = {\n",
    "        \"1_META_INFO\": {\n",
    "            \"timestamp_end\": datetime.now().isoformat(),\n",
    "            \"model_shortname\": MODEL_SHORTNAME,\n",
    "            \"seed\": TRAINING_SEED,\n",
    "            \"task\": \"M1 Detection - Training Loop\"\n",
    "        },\n",
    "        \"4_TRAINING_PERFORMANCE\": {\n",
    "            \"total_duration_minutes\": total_duration,\n",
    "            \"final_training_loss\": final_train_loss,\n",
    "            \"global_steps\": global_steps_done,\n",
    "            \"epochs\": training_args.num_train_epochs\n",
    "        },\n",
    "        \"5_LORA_PARAMS\": peft_config_data,\n",
    "        \"7_ARTIFACTS\": {\n",
    "            \"checkpoints_location\": \"Inside ZIP archive\",\n",
    "            \"zip_path\": f\"{ZIP_FULL_PATH}.zip\"\n",
    "        },\n",
    "        \"8_FULL_LOG_HISTORY\": getattr(trainer.state, \"log_history\", [])\n",
    "    }\n",
    "\n",
    "    with open(REPORT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(full_report, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"üì¶ Compressione ZIP in corso (attendere)...\")\n",
    "    shutil.make_archive(\n",
    "        base_name=ZIP_FULL_PATH, \n",
    "        format='zip', \n",
    "        root_dir=OUTPUT_DIR\n",
    "    )\n",
    "    print(f\"   -> ZIP creato: {ZIP_FULL_PATH}.zip\")\n",
    "\n",
    "    # --- E. PULIZIA MEMORIA ---\n",
    "    print(f\"üßπ Pulizia VRAM per il prossimo seed...\")\n",
    "    try:\n",
    "        del model\n",
    "        del trainer\n",
    "        del tokenizer\n",
    "    except: pass\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"‚ú® Ambiente pulito.\\n\")\n",
    "\n",
    "print(\"\\nüéâ TUTTE LE RUN PER M1 SONO COMPLETATE CON SUCCESSO!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d99a1ae-b0a2-4081-8b64-bde7934e9cff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformers (Python 3.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
