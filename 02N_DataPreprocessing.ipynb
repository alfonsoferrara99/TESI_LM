{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7be64fd-4e8a-461a-9916-269602fb7a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup completato.\n",
      "üìÇ Output HuggingFace: DATASET_ITA/PROCESSED_DATA/HF_DATASETS\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELLA 1: SETUP, LIBRERIE E CONTROL CENTER\n",
    "# ==============================================================================\n",
    "\n",
    "# Installazione librerie base\n",
    "!pip install -q transformers torch datasets\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- CONTROL CENTER ---\n",
    "CONFIG = {\n",
    "    # Percorsi di Input (Creati nel Notebook 1)\n",
    "    \"SPLITS_DIR\": os.path.join(\"DATASET_ITA\", \"PROCESSED_DATA\", \"splits\"),\n",
    "    \"HF_OUT_DIR\": os.path.join(\"DATASET_ITA\", \"PROCESSED_DATA\", \"HF_DATASETS\"),\n",
    "    \"FRAMES_BASE_DIR\": os.path.join(\"DATASET_ITA\", \"PROCESSED_DATA\", \"frames\"),\n",
    "    \n",
    "    # Label Mapping per M2 (Classification)\n",
    "    \"LABEL_MAP\": {\n",
    "        \"flaming\": 1,\n",
    "        \"denigration\": 2,\n",
    "        \"sexual\": 3,\n",
    "        \"racism\": 4\n",
    "    },\n",
    "    \n",
    "    # SEED (Per riproducibilit√† mescolamento)\n",
    "    \"SAMPLING_SEED\": 123\n",
    "}\n",
    "\n",
    "# Creazione cartelle output\n",
    "os.makedirs(CONFIG[\"HF_OUT_DIR\"], exist_ok=True)\n",
    "\n",
    "# Setup Seed Globale\n",
    "np.random.seed(CONFIG[\"SAMPLING_SEED\"])\n",
    "import random\n",
    "random.seed(CONFIG[\"SAMPLING_SEED\"])\n",
    "\n",
    "print(f\"‚úÖ Setup completato.\")\n",
    "print(f\"üìÇ Output HuggingFace: {CONFIG['HF_OUT_DIR']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82f85ea3-f180-48d8-b3bf-0036601f2797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Caricamento Dataset Master (output del Notebook 1)...\n",
      "   ‚úÖ TRAIN: Caricato con successo (3466 righe)\n",
      "   ‚úÖ VAL  : Caricato con successo (710 righe)\n",
      "   ‚úÖ TEST : Caricato con successo (855 righe)\n",
      "\n",
      "üìä ANALISI PRELIMINARE TRAIN SET:\n",
      "   üîπ Totale Commenti: 3466\n",
      "   üîπ Video Unici:     28\n",
      "\n",
      "   üìâ Distribuzione Classi Originale:\n",
      "Type\n",
      "none           2597\n",
      "denigration     616\n",
      "sexual          185\n",
      "flaming          50\n",
      "racism           18\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELLA 2: CARICAMENTO DATASET MASTER\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"‚è≥ Caricamento Dataset Master (output del Notebook 1)...\")\n",
    "\n",
    "# Dizionario per iterare sui file\n",
    "split_files = {\n",
    "    \"TRAIN\": \"master_train.csv\",\n",
    "    \"VAL\":   \"master_val.csv\",\n",
    "    \"TEST\":  \"master_test.csv\"\n",
    "}\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "try:\n",
    "    for split_name, filename in split_files.items():\n",
    "        file_path = os.path.join(CONFIG[\"SPLITS_DIR\"], filename)\n",
    "        \n",
    "        # Carichiamo specificando il separatore ';' e che video_id √® stringa\n",
    "        df = pd.read_csv(file_path, sep=';', dtype={'video_id': str})\n",
    "        \n",
    "        # Normalizzazione Label: tutto minuscolo e senza spazi extra\n",
    "        if 'Type' in df.columns:\n",
    "            df['Type'] = df['Type'].astype(str).str.lower().str.strip()\n",
    "            \n",
    "        datasets[split_name] = df\n",
    "        print(f\"   ‚úÖ {split_name:<5}: Caricato con successo ({len(df)} righe)\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\n‚ùå ERRORE: Non trovo il file {e.filename}\")\n",
    "    print(\"   Suggerimento: Hai eseguito il Notebook 1 e salvato i file in PROCESSED_DATA/splits?\")\n",
    "    raise\n",
    "\n",
    "# Assegnazione alle variabili globali per comodit√†\n",
    "df_train = datasets[\"TRAIN\"]\n",
    "df_val   = datasets[\"VAL\"]\n",
    "df_test  = datasets[\"TEST\"]\n",
    "\n",
    "# Report preliminare sul Train Set (quello su cui lavoreremo ora)\n",
    "print(f\"\\nüìä ANALISI PRELIMINARE TRAIN SET:\")\n",
    "print(f\"   üîπ Totale Commenti: {len(df_train)}\")\n",
    "print(f\"   üîπ Video Unici:     {df_train['video_id'].nunique()}\")\n",
    "print(\"\\n   üìâ Distribuzione Classi Originale:\")\n",
    "print(df_train['Type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cca9be9f-f86c-4b15-93ef-12ed5d9d4eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ PREPARAZIONE DATASET M1 - UNDERSAMPLING AVANZATO (None only)\n",
      "üîπ Commenti 'None' iniziali: 2597\n",
      "üßπ Applicazione filtri di qualit√† semantica sui commenti 'None'...\n",
      "   üîª Rimossi: 1093 commenti (42.1%)\n",
      "   ‚úÖ Commenti 'None' validi rimasti: 1504\n",
      "\n",
      "üìä DISTRIBUZIONE FINALE DATASET M1\n",
      "   üßÆ Totale righe: 2373\n",
      "   üü¢ Safe (0):  1504 (63.4%)\n",
      "   üî¥ Bully (1): 869 (36.6%)\n",
      "   ‚öñÔ∏è  Rapporto Safe/Bully: 1.73 : 1\n",
      "‚úÖ Dataset M1 pronto per il training\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELLA UNICA: DATASET M1 CREATION (UNDERSAMPLING INTELLIGENTE 'NONE')\n",
    "# ==============================================================================\n",
    "\n",
    "import re\n",
    "\n",
    "print(\"üöÄ PREPARAZIONE DATASET M1 - UNDERSAMPLING AVANZATO (None only)\")\n",
    "\n",
    "TEXT_COL = \"Comment\"\n",
    "\n",
    "# --- 1. Separazione iniziale ---\n",
    "df_bully = df_train[df_train[\"Type\"] != \"none\"].copy()\n",
    "df_none = df_train[df_train[\"Type\"] == \"none\"].copy()\n",
    "\n",
    "start_none = len(df_none)\n",
    "print(f\"üîπ Commenti 'None' iniziali: {start_none}\")\n",
    "\n",
    "# --- 2. Definizione filtri ---\n",
    "\n",
    "spam_keywords = {\n",
    "    \"first\", \"primo\", \"second\", \"secondo\",\n",
    "    \"like\", \"likes\", \"follow\", \"segui\",\n",
    "    \"sub\", \"iscriviti\", \"views\", \"visualizzazioni\",\n",
    "    \"edit\", \"parte\", \"pt\", \"up\", \"upp\", \"pls\", \"plz\", \"please\",\n",
    "    \"push\", \"boost\", \"spam\", \"link\", \"bio\", \"profilo\", \"dm\", \"priv\", \"pvt\"\n",
    "}\n",
    "\n",
    "social_noise_words = {\n",
    "    \"ahah\", \"haha\", \"lol\", \"ok\", \"boh\", \"mah\",\n",
    "    \"nice\", \"bella\", \"top\", \"grande\"\n",
    "}\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(r\"\\w+\", text.lower())\n",
    "\n",
    "def is_exact_spam(text):\n",
    "    if not isinstance(text, str):\n",
    "        return True\n",
    "    clean = re.sub(r\"[^\\w\\s]\", \"\", text.lower().strip())\n",
    "    return clean in spam_keywords\n",
    "\n",
    "def is_only_emoji_or_punct(text):\n",
    "    if not isinstance(text, str):\n",
    "        return True\n",
    "    return not any(char.isalnum() for char in text)\n",
    "\n",
    "def is_too_short(text, min_tokens=3):\n",
    "    return len(tokenize(text)) < min_tokens\n",
    "\n",
    "def is_spam_containment_short(text, max_tokens=3):\n",
    "    tokens = tokenize(text)\n",
    "    if len(tokens) > max_tokens:\n",
    "        return False\n",
    "    return any(tok in spam_keywords for tok in tokens)\n",
    "\n",
    "def is_social_noise(text):\n",
    "    tokens = tokenize(text)\n",
    "    return len(tokens) == 1 and tokens[0] in social_noise_words\n",
    "\n",
    "def is_low_quality_none(text):\n",
    "    return (\n",
    "        is_exact_spam(text) or\n",
    "        is_only_emoji_or_punct(text) or\n",
    "        is_too_short(text) or\n",
    "        is_spam_containment_short(text) or\n",
    "        is_social_noise(text)\n",
    "    )\n",
    "\n",
    "# --- 3. Applicazione filtri ---\n",
    "print(\"üßπ Applicazione filtri di qualit√† semantica sui commenti 'None'...\")\n",
    "mask_keep = ~df_none[TEXT_COL].apply(is_low_quality_none)\n",
    "df_none_clean = df_none[mask_keep].copy()\n",
    "\n",
    "removed = start_none - len(df_none_clean)\n",
    "perc_removed = (removed / start_none) * 100\n",
    "\n",
    "print(f\"   üîª Rimossi: {removed} commenti ({perc_removed:.1f}%)\")\n",
    "print(f\"   ‚úÖ Commenti 'None' validi rimasti: {len(df_none_clean)}\")\n",
    "\n",
    "# --- 4. Assegnazione label binaria ---\n",
    "df_none_clean[\"binary_label\"] = 0\n",
    "df_bully[\"binary_label\"] = 1\n",
    "\n",
    "# --- 5. Merge e shuffle ---\n",
    "df_train_M1 = pd.concat([df_none_clean, df_bully]).sample(\n",
    "    frac=1,\n",
    "    random_state=CONFIG[\"SAMPLING_SEED\"]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# --- 6. Report finale ---\n",
    "n_safe = len(df_none_clean)\n",
    "n_bully = len(df_bully)\n",
    "total = len(df_train_M1)\n",
    "\n",
    "print(\"\\nüìä DISTRIBUZIONE FINALE DATASET M1\")\n",
    "print(f\"   üßÆ Totale righe: {total}\")\n",
    "print(f\"   üü¢ Safe (0):  {n_safe} ({n_safe/total:.1%})\")\n",
    "print(f\"   üî¥ Bully (1): {n_bully} ({n_bully/total:.1%})\")\n",
    "print(f\"   ‚öñÔ∏è  Rapporto Safe/Bully: {n_safe / n_bully:.2f} : 1\")\n",
    "\n",
    "# --- 7. Check di integrit√† ---\n",
    "assert not df_train_M1[\"video_id\"].isnull().any(), \"Errore: video_id nulli\"\n",
    "assert TEXT_COL in df_train_M1.columns, \"Errore: colonna testo mancante\"\n",
    "\n",
    "print(\"‚úÖ Dataset M1 pronto per il training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5c1679e-58bb-469f-83f6-2e4cbad8d5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è COSTRUZIONE DATASET M2 (Classification)...\n",
      "¬† ¬†üîπ Base M2 (Solo Offensivi Reali): 869 righe\n",
      "\n",
      "üöÄ [1/2] Avvio Augmentation Semantica...\n",
      "¬† ¬†‚úÖ Aggiunte 30 varianti semantiche.\n",
      "\n",
      "üöÄ [2/2] Avvio Soft Oversampling (Solo Classi Rare)...\n",
      "¬† ¬†üìä Pre-Oversampling: Racism=26, Flaming=64\n",
      "¬† ¬†‚úÖ Aggiunte 68 righe di supporto (Moltiplicatore 2x).\n",
      "\n",
      "‚úÖ DATASET M2 COMPLETATO.\n",
      "¬† ¬†Totale Righe: 967\n",
      "¬† ¬†Distribuzione Classi Finale:\n",
      "Type\n",
      "denigration    616\n",
      "sexual         193\n",
      "flaming        114\n",
      "racism          44\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELLA 4: DATASET M2 (STRATEGIA SCIENTIFICA: SEMANTIC AUG + SOFT BOOST 2x)\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n‚öôÔ∏è COSTRUZIONE DATASET M2 (Classification)...\")\n",
    "\n",
    "# 1. Filtro Base: Teniamo solo i commenti offensivi dal TRAIN\n",
    "df_train_M2 = df_train[df_train['Type'] != 'none'].copy()\n",
    "\n",
    "# Inizializziamo subito le colonne di supporto per evitare errori di tipo misto\n",
    "df_train_M2['is_synthetic'] = False\n",
    "df_train_M2['aug_method'] = 'original'\n",
    "\n",
    "print(f\"¬† ¬†üîπ Base M2 (Solo Offensivi Reali): {len(df_train_M2)} righe\")\n",
    "\n",
    "# --- FASE A: AUGMENTATION SEMANTICA ---\n",
    "AUG_MAP = {\n",
    "    \"il sardo √® pi√π comprensibile\": [\"persino il calabrese si capisce pi√π di te\", \"a confronto il sardo sembra italiano perfetto\", \"parli peggio di un sardo stretto\"],\n",
    "    \"il barese √® pi√π comprensibile\": [\"anche il barese si capisce meglio di te\", \"il barese √® molto pi√π chiaro del tuo modo di parlare\"],\n",
    "    \"sto rivalutando il sardo\": [\"a confronto sto rivalutando il sardo\", \"dopo averti sentito il sardo sembra chiaro\"],\n",
    "    \"pari na zingara\": [\"sembri proprio una zingara\", \"pari davvero una zingara\", \"una zingarella\"],\n",
    "    \"DAI VAI A PAGARE LE PIZZE\": [\"vai a lavorare invece di scroccare come una zingara\", \"paga le pizze invece di scroccare, zingara\"],\n",
    "    \"sposa Putin\": [\"vai a vivere con Putin\", \"vedrei bene Putin per te...\", \"sposati Putin e sparisci\"],\n",
    "    \"ma ti rendi conto di essere ridicola\": [\"ti rendi conto di quanto sei ridicola?\", \"ma ti accorgi di essere ridicola?\"],\n",
    "    \"sempre ridicolo √®\": [\"ridicoloo\", \"sempre ridicolo\"],\n",
    "    \"ma chi ti conosce\": [\"ma chi ti conosce davvero?\", \"ma chi vuoi che ti conosca\"],\n",
    "    \"Ma solo io la vedo cretina\": [\"sono l‚Äôunico a vederla cretina?\", \"solo io la trovo cretina?\"],\n",
    "    \"che morta di fame\": [\"proprio una morta di fame\", \"una vera morta di fame\"],\n",
    "    \"vai bene come mocio\": [\"saresti utile come mocio\", \"vai bene giusto come mocio\"],\n",
    "    \"Influencer di sto cazzo\": [\"influencer di merda, non siete nessuno!\", \"siete dei pezzenti, altro che influencer\"],\n",
    "    \"foto del suo fondoschiena\": [\"solo foto del suo fondoschiena?\", \"ancora foto del suo sedere?\"],\n",
    "    \"fa vedere il culo\": [\"influencer vuol dire mostrare il culo in ogni video\", \"fa vedere il sedere in ogni video\"],\n",
    "    \"il tuo latoB tanto vale\": [\"il tuo lato B vale giusto una pizza\", \"per quel lato B non pagherei nulla\"],\n",
    "    \"se ti pago la pizza\": [\"se ti offro la pizza cosa mi dai in cambio?\", \"se pago la pizza per te cosa ottengo?\"]\n",
    "}\n",
    "\n",
    "new_rows = []\n",
    "print(\"\\nüöÄ [1/2] Avvio Augmentation Semantica...\")\n",
    "\n",
    "for search_key, variants in AUG_MAP.items():\n",
    "    # Cerca la frase nei commenti (case insensitive)\n",
    "    mask = df_train_M2['Comment'].astype(str).str.lower().str.contains(search_key.lower().strip(), regex=False)\n",
    "    matches = df_train_M2[mask]\n",
    "    \n",
    "    if len(matches) > 0:\n",
    "        # Prende il primo match per copiare i metadati\n",
    "        original_row = matches.iloc[0]\n",
    "        for v in variants:\n",
    "            new_rows.append({\n",
    "                'Comment': v,\n",
    "                'Type': original_row['Type'],\n",
    "                'video_id': original_row['video_id'],\n",
    "                'is_synthetic': True,\n",
    "                'aug_method': 'semantic_rule'\n",
    "            })\n",
    "\n",
    "if new_rows:\n",
    "    df_aug_semantic = pd.DataFrame(new_rows)\n",
    "    df_train_M2 = pd.concat([df_train_M2, df_aug_semantic], ignore_index=True)\n",
    "    print(f\"¬† ¬†‚úÖ Aggiunte {len(df_aug_semantic)} varianti semantiche.\")\n",
    "else:\n",
    "    print(\"¬† ¬†‚ö†Ô∏è Nessuna variante semantica generata.\")\n",
    "\n",
    "# --- FASE B: SOFT OVERSAMPLING 2x ---\n",
    "print(\"\\nüöÄ [2/2] Avvio Soft Oversampling (Solo Classi Rare)...\")\n",
    "\n",
    "RARE_CLASSES = ['racism', 'flaming']\n",
    "MULTIPLIER = 2 \n",
    "\n",
    "oversample_rows = []\n",
    "# Calcoliamo i conteggi sul dataset corrente (che include gi√† l'augmentation semantica!)\n",
    "counts = df_train_M2['Type'].value_counts()\n",
    "print(f\"¬† ¬†üìä Pre-Oversampling: Racism={counts.get('racism',0)}, Flaming={counts.get('flaming',0)}\")\n",
    "\n",
    "# Iteriamo solo sulle righe ORIGINALI (non quelle appena create) per evitare di duplicare i duplicati all'infinito\n",
    "# Filtriamo: is_synthetic == False\n",
    "originals_only = df_train_M2[df_train_M2['is_synthetic'] == False]\n",
    "\n",
    "for _, row in originals_only.iterrows():\n",
    "    if row['Type'] in RARE_CLASSES:\n",
    "        # Creiamo copie\n",
    "        for _ in range(MULTIPLIER - 1):\n",
    "            row_copy = row.copy()\n",
    "            # Qui forziamo il casting a object/bool se necessario, ma dato che creiamo un nuovo DF alla fine, Pandas gestir√† i tipi\n",
    "            row_copy['is_synthetic'] = True\n",
    "            row_copy['aug_method'] = 'soft_oversampling'\n",
    "            oversample_rows.append(row_copy)\n",
    "\n",
    "if oversample_rows:\n",
    "    df_boost = pd.DataFrame(oversample_rows)\n",
    "    df_train_M2_FINAL = pd.concat([df_train_M2, df_boost], ignore_index=True)\n",
    "    print(f\"¬† ¬†‚úÖ Aggiunte {len(df_boost)} righe di supporto (Moltiplicatore {MULTIPLIER}x).\")\n",
    "else:\n",
    "    df_train_M2_FINAL = df_train_M2.copy()\n",
    "\n",
    "# Shuffle finale\n",
    "df_train_M2_FINAL = df_train_M2_FINAL.sample(frac=1, random_state=CONFIG[\"SAMPLING_SEED\"]).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n‚úÖ DATASET M2 COMPLETATO.\")\n",
    "print(f\"¬† ¬†Totale Righe: {len(df_train_M2_FINAL)}\")\n",
    "print(f\"¬† ¬†Distribuzione Classi Finale:\\n{df_train_M2_FINAL['Type'].value_counts()}\")\n",
    "\n",
    "# Check Multimodale\n",
    "if 'video_id' not in df_train_M2_FINAL.columns:\n",
    "    raise ValueError(\"‚ùå ERRORE CRITICO: video_id perso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b20a2b4-9137-4838-8588-9faca5c5fe08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ AVVIO ESPORTAZIONE HUGGING FACE DATASETS...\n",
      "\n",
      "üèóÔ∏è  Costruzione Dataset HF: M1_detection (detection)...\n",
      "   üîπ TRAIN: 2373 esempi (Skipped: 0)\n",
      "   üîπ VAL: 710 esempi (Skipped: 0)\n",
      "   üîπ TEST: 855 esempi (Skipped: 0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10bd9cfce2b249fba3ae6ac4a2cc5af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2373 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ad4f028cc3b4a18bdaa221f624c5492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/710 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08f6c7062b7740feb48f03db3ae2fd24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/855 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Salvato in: DATASET_ITA/PROCESSED_DATA/HF_DATASETS/M1_detection\n",
      "\n",
      "üèóÔ∏è  Costruzione Dataset HF: M2_classification (classification)...\n",
      "   üîπ TRAIN: 967 esempi (Skipped: 0)\n",
      "   üîπ VAL: 168 esempi (Skipped: 0)\n",
      "   üîπ TEST: 211 esempi (Skipped: 0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "084deebccc504dd5b456f0f2f4f88668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/967 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f8dbde02ca4fa897a68745d5147488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/168 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b362ee4b87554d01936f9557b4916e8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/211 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Salvato in: DATASET_ITA/PROCESSED_DATA/HF_DATASETS/M2_classification\n",
      "\n",
      "üéâ FINE NOTEBOOK 2. I dataset sono pronti e puliti in: DATASET_ITA/PROCESSED_DATA/HF_DATASETS\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELLA 5: EXPORT DATASETS (M1 e M2) IN FORMATO HUGGING FACE\n",
    "# ==============================================================================\n",
    "from datasets import Dataset, DatasetDict\n",
    "import os\n",
    "\n",
    "# Recuperiamo i percorsi dalla CONFIG globale\n",
    "HF_OUT_DIR = CONFIG[\"HF_OUT_DIR\"]\n",
    "FRAMES_BASE_DIR = CONFIG[\"FRAMES_BASE_DIR\"]\n",
    "\n",
    "# Mapping M2 (Classification)\n",
    "LABEL_MAP_M2 = CONFIG[\"LABEL_MAP\"]\n",
    "\n",
    "def row_to_conversation(row, mode=\"detection\"):\n",
    "    \"\"\"\n",
    "    Converte una riga in formato Chat.\n",
    "    Gestisce automaticamente la mancanza di 'binary_label' in Val/Test.\n",
    "    \"\"\"\n",
    "    video_id = str(row['video_id'])\n",
    "    text_comment = row['Comment']\n",
    "    \n",
    "    # 1. Recupero Frame\n",
    "    video_folder = os.path.join(FRAMES_BASE_DIR, video_id)\n",
    "    image_paths = []\n",
    "    \n",
    "    if os.path.exists(video_folder):\n",
    "        frames = sorted([f for f in os.listdir(video_folder) if f.lower().endswith(\".jpg\")])\n",
    "        frames = frames[:3] \n",
    "        image_paths = [os.path.abspath(os.path.join(video_folder, f)) for f in frames]\n",
    "    \n",
    "    if not image_paths: return None \n",
    "\n",
    "    # 2. USER CONTENT (DATA ONLY)\n",
    "    user_content = []\n",
    "    for img_path in image_paths:\n",
    "        user_content.append({\"type\": \"image\", \"image\": img_path})\n",
    "    \n",
    "    # Prompt Pulito (Il system prompt sar√† aggiunto nel training)\n",
    "    clean_text = f\"Commento: \\\"{text_comment}\\\"\"\n",
    "    user_content.append({\"type\": \"text\", \"text\": clean_text})\n",
    "\n",
    "    # 3. LABEL ASSISTANT (LOGICA CORRETTA)\n",
    "    if mode == \"detection\":\n",
    "        # CASO M1:\n",
    "        # Se abbiamo gi√† la colonna binaria (Training Set), usiamola.\n",
    "        if 'binary_label' in row:\n",
    "            label = str(int(row['binary_label']))\n",
    "        else:\n",
    "            # Fallback per Val/Test che non hanno la colonna 'binary_label':\n",
    "            # Se Type √® 'none' -> 0, altrimenti -> 1\n",
    "            label = \"0\" if row['Type'] == 'none' else \"1\"\n",
    "            \n",
    "    elif mode == \"classification\":\n",
    "        # CASO M2:\n",
    "        if row['Type'] not in LABEL_MAP_M2: return None\n",
    "        label = str(LABEL_MAP_M2[row['Type']])\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    # Struttura Chat Finale\n",
    "    return [\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "        {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": label}]}\n",
    "    ]\n",
    "\n",
    "def create_and_save_hf_dataset(df_train, df_val, df_test, task_name, mode):\n",
    "    print(f\"\\nüèóÔ∏è  Costruzione Dataset HF: {task_name} ({mode})...\")\n",
    "    \n",
    "    splits = {\"train\": df_train, \"val\": df_val, \"test\": df_test}\n",
    "    ds_dict = {}\n",
    "    \n",
    "    for split_name, df in splits.items():\n",
    "        if df is None or len(df) == 0: continue\n",
    "            \n",
    "        data_list = []\n",
    "        skipped = 0\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            msgs = row_to_conversation(row, mode=mode)\n",
    "            if msgs:\n",
    "                data_list.append({\"messages\": msgs}) \n",
    "            else:\n",
    "                skipped += 1\n",
    "        \n",
    "        if data_list:\n",
    "            ds_dict[split_name] = Dataset.from_list(data_list)\n",
    "            print(f\"   üîπ {split_name.upper()}: {len(ds_dict[split_name])} esempi (Skipped: {skipped})\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è {split_name.upper()}: Nessun dato valido generato.\")\n",
    "\n",
    "    # Salvataggio\n",
    "    final_ds = DatasetDict(ds_dict)\n",
    "    save_path = os.path.join(HF_OUT_DIR, task_name)\n",
    "    final_ds.save_to_disk(save_path)\n",
    "    print(f\"   ‚úÖ Salvato in: {save_path}\")\n",
    "\n",
    "# --- ESECUZIONE ---\n",
    "print(\"üì¶ AVVIO ESPORTAZIONE HUGGING FACE DATASETS...\")\n",
    "\n",
    "# 1. TASK 1: DETECTION (M1)\n",
    "create_and_save_hf_dataset(\n",
    "    df_train_M1, df_val, df_test, \n",
    "    task_name=\"M1_detection\", \n",
    "    mode=\"detection\"\n",
    ")\n",
    "\n",
    "# 2. TASK 2: CLASSIFICATION (M2)\n",
    "df_val_m2 = df_val[df_val['Type'] != 'none'].copy()\n",
    "df_test_m2 = df_test[df_test['Type'] != 'none'].copy()\n",
    "\n",
    "create_and_save_hf_dataset(\n",
    "    df_train_M2_FINAL, df_val_m2, df_test_m2, \n",
    "    task_name=\"M2_classification\", \n",
    "    mode=\"classification\"\n",
    ")\n",
    "\n",
    "print(f\"\\nüéâ FINE NOTEBOOK 2. I dataset sono pronti e puliti in: {HF_OUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea94ae63-a5b1-41c3-aa92-9b69df530ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformers (Python 3.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
