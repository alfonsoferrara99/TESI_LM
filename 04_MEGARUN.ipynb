{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b0263df-a381-4cf8-acfb-cd96f7222ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pandas/core/computation/expressions.py:22: UserWarning: Pandas requires version '2.10.2' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Using MoE backend 'grouped_mm'\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "‚úÖ Ambiente Pronto e Pulito.\n",
      "   ‚Ä¢ GPU: Tesla V100S-PCIE-32GB\n",
      "   ‚Ä¢ PyTorch: 2.10.0+cu128\n",
      "   ‚Ä¢ Unsloth: 2026.2.1\n",
      "   ‚Ä¢ Transformers: 4.57.6\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELLA 0: SETUP TOTALE (MINIMAL & STABILE)\n",
    "# ==============================================================================\n",
    "import sys\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# 1. BLOCCO MODULI PROBLEMATICI\n",
    "sys.modules[\"vllm\"] = None\n",
    "sys.modules[\"vllm.sampling_params\"] = None\n",
    "\n",
    "print(\"‚è≥ Setup Ambiente in corso... (Attendere, output nascosto)\")\n",
    "\n",
    "# 2. INSTALLAZIONE & AGGIORNAMENTO SILENZIOSO\n",
    "# Scarica l'ultima versione di Unsloth da Git e aggiorna automaticamente \n",
    "# PyTorch e Transformers alle versioni pi√π recenti e compatibili.\n",
    "!pip install --upgrade --no-cache-dir --quiet \\\n",
    "    \"torch\" \"torchvision\" \"torchaudio\" \\\n",
    "    \"transformers\" \"trl\" \"peft\" \"accelerate\" \"bitsandbytes\" \\\n",
    "    \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" \\\n",
    "    \"unsloth_zoo @ git+https://github.com/unslothai/unsloth-zoo.git\" \\\n",
    "    \"pillow\" \"scikit-learn\" \"pandas\"\n",
    "\n",
    "# 3. VERIFICA E PULIZIA\n",
    "clear_output()\n",
    "\n",
    "import torch\n",
    "import unsloth\n",
    "import transformers\n",
    "from PIL import Image\n",
    "\n",
    "print(f\"‚úÖ Ambiente Pronto e Pulito.\")\n",
    "print(f\"   ‚Ä¢ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"   ‚Ä¢ PyTorch: {torch.__version__}\")\n",
    "print(f\"   ‚Ä¢ Unsloth: {unsloth.__version__}\")\n",
    "print(f\"   ‚Ä¢ Transformers: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d99a1ae-b0a2-4081-8b64-bde7934e9cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Caricamento Dataset HF M2 (Eseguito una volta sola)...\n",
      "üîÑ Formattazione Dataset M2...\n",
      "‚úÖ Dataset M2 Caricato e Formattato. Train: 967 | Val: 168\n",
      "\n",
      "üöÄ AVVIO SESSIONE DI TRAINING M2 SU 4 SEED: [101, 285, 3692, 92]\n",
      "\n",
      "############################################################\n",
      "üé¨ RUN 1/4 | SEED CORRENTE: 101\n",
      "############################################################\n",
      "üîí Fissaggio Seed Globali a 101...\n",
      "üìÇ Cartella Output Run: outputs/Qwen2.5-VL-M2-Classification_Seed_101\n",
      "‚è≥ Inizializzazione Modello M2 (Seed 101)...\n",
      "==((====))==  Unsloth 2026.2.1: Fast Qwen2_5_Vl patching. Transformers: 4.57.6. vLLM: 0.6.3.\n",
      "   \\\\   /|    Tesla V100S-PCIE-32GB. Num GPUs = 1. Max memory: 31.739 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 7.0. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "Unsloth: Model does not have a default image size - using 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Avvio Training M2 Seed 101...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 967 | Num Epochs = 5 | Total steps = 305\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 47,589,376 of 8,339,756,032 (0.57% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='305' max='305' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [305/305 2:29:30, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.222100</td>\n",
       "      <td>0.151002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.140900</td>\n",
       "      <td>0.127641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.137800</td>\n",
       "      <td>0.125928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.132000</td>\n",
       "      <td>0.125879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.142900</td>\n",
       "      <td>0.125613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Step:   10 | Epoch: 0.17 | Loss: 4.2133\n",
      "üìù Step:   20 | Epoch: 0.33 | Loss: 2.9486\n",
      "üìù Step:   30 | Epoch: 0.50 | Loss: 2.0132\n",
      "üìù Step:   40 | Epoch: 0.66 | Loss: 1.3382\n",
      "üìù Step:   50 | Epoch: 0.83 | Loss: 0.5922\n",
      "üìù Step:   60 | Epoch: 0.99 | Loss: 0.2221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Qwen2_5_VLForConditionalGeneration does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Step:   70 | Epoch: 1.15 | Loss: 0.1670\n",
      "üìù Step:   80 | Epoch: 1.31 | Loss: 0.1491\n",
      "üìù Step:   90 | Epoch: 1.48 | Loss: 0.1427\n",
      "üìù Step:  100 | Epoch: 1.64 | Loss: 0.1415\n",
      "üìù Step:  110 | Epoch: 1.81 | Loss: 0.1516\n",
      "üìù Step:  120 | Epoch: 1.98 | Loss: 0.1409\n",
      "üìù Step:  130 | Epoch: 2.13 | Loss: 0.1409\n",
      "üìù Step:  140 | Epoch: 2.30 | Loss: 0.1386\n",
      "üìù Step:  150 | Epoch: 2.46 | Loss: 0.1416\n",
      "üìù Step:  160 | Epoch: 2.63 | Loss: 0.1201\n",
      "üìù Step:  170 | Epoch: 2.79 | Loss: 0.1371\n",
      "üìù Step:  180 | Epoch: 2.96 | Loss: 0.1378\n",
      "üìù Step:  190 | Epoch: 3.12 | Loss: 0.1500\n",
      "üìù Step:  200 | Epoch: 3.28 | Loss: 0.1239\n",
      "üìù Step:  210 | Epoch: 3.45 | Loss: 0.1297\n",
      "üìù Step:  220 | Epoch: 3.61 | Loss: 0.1189\n",
      "üìù Step:  230 | Epoch: 3.78 | Loss: 0.1404\n",
      "üìù Step:  240 | Epoch: 3.94 | Loss: 0.1320\n",
      "üìù Step:  250 | Epoch: 4.10 | Loss: 0.1326\n",
      "üìù Step:  260 | Epoch: 4.26 | Loss: 0.1183\n",
      "üìù Step:  270 | Epoch: 4.43 | Loss: 0.1305\n",
      "üìù Step:  280 | Epoch: 4.60 | Loss: 0.1189\n",
      "üìù Step:  290 | Epoch: 4.76 | Loss: 0.1292\n",
      "üìù Step:  300 | Epoch: 4.93 | Loss: 0.1429\n",
      "‚úÖ Training M2 Finito. Durata: 149.95 min | Loss: 0.4808\n",
      "üíæ Salvataggio Artifacts M2...\n",
      "üì¶ Compressione ZIP in corso (attendere)...\n",
      "   -> ZIP creato: outputs/Qwen2.5-VL-M2-Classification_Seed_101_FULL_CHECKPOINTS.zip\n",
      "üßπ Pulizia VRAM per il prossimo seed...\n",
      "‚ú® Ambiente pulito.\n",
      "\n",
      "\n",
      "############################################################\n",
      "üé¨ RUN 2/4 | SEED CORRENTE: 285\n",
      "############################################################\n",
      "üîí Fissaggio Seed Globali a 285...\n",
      "üìÇ Cartella Output Run: outputs/Qwen2.5-VL-M2-Classification_Seed_285\n",
      "‚è≥ Inizializzazione Modello M2 (Seed 285)...\n",
      "==((====))==  Unsloth 2026.2.1: Fast Qwen2_5_Vl patching. Transformers: 4.57.6. vLLM: 0.6.3.\n",
      "   \\\\   /|    Tesla V100S-PCIE-32GB. Num GPUs = 1. Max memory: 31.739 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 7.0. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "Unsloth: Model does not have a default image size - using 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Avvio Training M2 Seed 285...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 967 | Num Epochs = 5 | Total steps = 305\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 47,589,376 of 8,339,756,032 (0.57% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='305' max='305' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [305/305 2:30:20, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.150765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.140800</td>\n",
       "      <td>0.127636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.137800</td>\n",
       "      <td>0.126164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.132000</td>\n",
       "      <td>0.126266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.143000</td>\n",
       "      <td>0.126116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Step:   10 | Epoch: 0.17 | Loss: 4.2142\n",
      "üìù Step:   20 | Epoch: 0.33 | Loss: 2.9542\n",
      "üìù Step:   30 | Epoch: 0.50 | Loss: 2.0189\n",
      "üìù Step:   40 | Epoch: 0.66 | Loss: 1.3365\n",
      "üìù Step:   50 | Epoch: 0.83 | Loss: 0.5847\n",
      "üìù Step:   60 | Epoch: 0.99 | Loss: 0.2200\n",
      "üìù Step:   70 | Epoch: 1.15 | Loss: 0.1670\n",
      "üìù Step:   80 | Epoch: 1.31 | Loss: 0.1489\n",
      "üìù Step:   90 | Epoch: 1.48 | Loss: 0.1426\n",
      "üìù Step:  100 | Epoch: 1.64 | Loss: 0.1414\n",
      "üìù Step:  110 | Epoch: 1.81 | Loss: 0.1515\n",
      "üìù Step:  120 | Epoch: 1.98 | Loss: 0.1408\n",
      "üìù Step:  130 | Epoch: 2.13 | Loss: 0.1408\n",
      "üìù Step:  140 | Epoch: 2.30 | Loss: 0.1385\n",
      "üìù Step:  150 | Epoch: 2.46 | Loss: 0.1416\n",
      "üìù Step:  160 | Epoch: 2.63 | Loss: 0.1201\n",
      "üìù Step:  170 | Epoch: 2.79 | Loss: 0.1371\n",
      "üìù Step:  180 | Epoch: 2.96 | Loss: 0.1378\n",
      "üìù Step:  190 | Epoch: 3.12 | Loss: 0.1500\n",
      "üìù Step:  200 | Epoch: 3.28 | Loss: 0.1238\n",
      "üìù Step:  210 | Epoch: 3.45 | Loss: 0.1296\n",
      "üìù Step:  220 | Epoch: 3.61 | Loss: 0.1189\n",
      "üìù Step:  230 | Epoch: 3.78 | Loss: 0.1403\n",
      "üìù Step:  240 | Epoch: 3.94 | Loss: 0.1320\n",
      "üìù Step:  250 | Epoch: 4.10 | Loss: 0.1324\n",
      "üìù Step:  260 | Epoch: 4.26 | Loss: 0.1182\n",
      "üìù Step:  270 | Epoch: 4.43 | Loss: 0.1304\n",
      "üìù Step:  280 | Epoch: 4.60 | Loss: 0.1188\n",
      "üìù Step:  290 | Epoch: 4.76 | Loss: 0.1292\n",
      "üìù Step:  300 | Epoch: 4.93 | Loss: 0.1430\n",
      "‚úÖ Training M2 Finito. Durata: 150.58 min | Loss: 0.4808\n",
      "üíæ Salvataggio Artifacts M2...\n",
      "üì¶ Compressione ZIP in corso (attendere)...\n",
      "   -> ZIP creato: outputs/Qwen2.5-VL-M2-Classification_Seed_285_FULL_CHECKPOINTS.zip\n",
      "üßπ Pulizia VRAM per il prossimo seed...\n",
      "‚ú® Ambiente pulito.\n",
      "\n",
      "\n",
      "############################################################\n",
      "üé¨ RUN 3/4 | SEED CORRENTE: 3692\n",
      "############################################################\n",
      "üîí Fissaggio Seed Globali a 3692...\n",
      "üìÇ Cartella Output Run: outputs/Qwen2.5-VL-M2-Classification_Seed_3692\n",
      "‚è≥ Inizializzazione Modello M2 (Seed 3692)...\n",
      "==((====))==  Unsloth 2026.2.1: Fast Qwen2_5_Vl patching. Transformers: 4.57.6. vLLM: 0.6.3.\n",
      "   \\\\   /|    Tesla V100S-PCIE-32GB. Num GPUs = 1. Max memory: 31.739 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 7.0. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "Unsloth: Model does not have a default image size - using 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Avvio Training M2 Seed 3692...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 967 | Num Epochs = 5 | Total steps = 305\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 47,589,376 of 8,339,756,032 (0.57% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='305' max='305' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [305/305 2:29:47, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.219000</td>\n",
       "      <td>0.150457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.140800</td>\n",
       "      <td>0.127665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.137800</td>\n",
       "      <td>0.126086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.131900</td>\n",
       "      <td>0.126161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.142800</td>\n",
       "      <td>0.126079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Step:   10 | Epoch: 0.17 | Loss: 4.2152\n",
      "üìù Step:   20 | Epoch: 0.33 | Loss: 2.9527\n",
      "üìù Step:   30 | Epoch: 0.50 | Loss: 2.0147\n",
      "üìù Step:   40 | Epoch: 0.66 | Loss: 1.3354\n",
      "üìù Step:   50 | Epoch: 0.83 | Loss: 0.5827\n",
      "üìù Step:   60 | Epoch: 0.99 | Loss: 0.2190\n",
      "üìù Step:   70 | Epoch: 1.15 | Loss: 0.1668\n",
      "üìù Step:   80 | Epoch: 1.31 | Loss: 0.1488\n",
      "üìù Step:   90 | Epoch: 1.48 | Loss: 0.1426\n",
      "üìù Step:  100 | Epoch: 1.64 | Loss: 0.1414\n",
      "üìù Step:  110 | Epoch: 1.81 | Loss: 0.1514\n",
      "üìù Step:  120 | Epoch: 1.98 | Loss: 0.1408\n",
      "üìù Step:  130 | Epoch: 2.13 | Loss: 0.1407\n",
      "üìù Step:  140 | Epoch: 2.30 | Loss: 0.1385\n",
      "üìù Step:  150 | Epoch: 2.46 | Loss: 0.1416\n",
      "üìù Step:  160 | Epoch: 2.63 | Loss: 0.1200\n",
      "üìù Step:  170 | Epoch: 2.79 | Loss: 0.1372\n",
      "üìù Step:  180 | Epoch: 2.96 | Loss: 0.1378\n",
      "üìù Step:  190 | Epoch: 3.12 | Loss: 0.1500\n",
      "üìù Step:  200 | Epoch: 3.28 | Loss: 0.1239\n",
      "üìù Step:  210 | Epoch: 3.45 | Loss: 0.1296\n",
      "üìù Step:  220 | Epoch: 3.61 | Loss: 0.1189\n",
      "üìù Step:  230 | Epoch: 3.78 | Loss: 0.1402\n",
      "üìù Step:  240 | Epoch: 3.94 | Loss: 0.1319\n",
      "üìù Step:  250 | Epoch: 4.10 | Loss: 0.1324\n",
      "üìù Step:  260 | Epoch: 4.26 | Loss: 0.1182\n",
      "üìù Step:  270 | Epoch: 4.43 | Loss: 0.1303\n",
      "üìù Step:  280 | Epoch: 4.60 | Loss: 0.1188\n",
      "üìù Step:  290 | Epoch: 4.76 | Loss: 0.1292\n",
      "üìù Step:  300 | Epoch: 4.93 | Loss: 0.1428\n",
      "‚úÖ Training M2 Finito. Durata: 150.03 min | Loss: 0.4805\n",
      "üíæ Salvataggio Artifacts M2...\n",
      "üì¶ Compressione ZIP in corso (attendere)...\n",
      "   -> ZIP creato: outputs/Qwen2.5-VL-M2-Classification_Seed_3692_FULL_CHECKPOINTS.zip\n",
      "üßπ Pulizia VRAM per il prossimo seed...\n",
      "‚ú® Ambiente pulito.\n",
      "\n",
      "\n",
      "############################################################\n",
      "üé¨ RUN 4/4 | SEED CORRENTE: 92\n",
      "############################################################\n",
      "üîí Fissaggio Seed Globali a 92...\n",
      "üìÇ Cartella Output Run: outputs/Qwen2.5-VL-M2-Classification_Seed_92\n",
      "‚è≥ Inizializzazione Modello M2 (Seed 92)...\n",
      "==((====))==  Unsloth 2026.2.1: Fast Qwen2_5_Vl patching. Transformers: 4.57.6. vLLM: 0.6.3.\n",
      "   \\\\   /|    Tesla V100S-PCIE-32GB. Num GPUs = 1. Max memory: 31.739 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 7.0. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "Unsloth: Model does not have a default image size - using 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Avvio Training M2 Seed 92...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 967 | Num Epochs = 5 | Total steps = 305\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 47,589,376 of 8,339,756,032 (0.57% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='305' max='305' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [305/305 2:30:20, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.220400</td>\n",
       "      <td>0.151645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.140800</td>\n",
       "      <td>0.127651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.137700</td>\n",
       "      <td>0.125958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.131800</td>\n",
       "      <td>0.125897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.142800</td>\n",
       "      <td>0.125715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Step:   10 | Epoch: 0.17 | Loss: 4.2155\n",
      "üìù Step:   20 | Epoch: 0.33 | Loss: 2.9547\n",
      "üìù Step:   30 | Epoch: 0.50 | Loss: 2.0164\n",
      "üìù Step:   40 | Epoch: 0.66 | Loss: 1.3349\n",
      "üìù Step:   50 | Epoch: 0.83 | Loss: 0.5830\n",
      "üìù Step:   60 | Epoch: 0.99 | Loss: 0.2204\n",
      "üìù Step:   70 | Epoch: 1.15 | Loss: 0.1674\n",
      "üìù Step:   80 | Epoch: 1.31 | Loss: 0.1488\n",
      "üìù Step:   90 | Epoch: 1.48 | Loss: 0.1426\n",
      "üìù Step:  100 | Epoch: 1.64 | Loss: 0.1414\n",
      "üìù Step:  110 | Epoch: 1.81 | Loss: 0.1515\n",
      "üìù Step:  120 | Epoch: 1.98 | Loss: 0.1408\n",
      "üìù Step:  130 | Epoch: 2.13 | Loss: 0.1408\n",
      "üìù Step:  140 | Epoch: 2.30 | Loss: 0.1385\n",
      "üìù Step:  150 | Epoch: 2.46 | Loss: 0.1416\n",
      "üìù Step:  160 | Epoch: 2.63 | Loss: 0.1200\n",
      "üìù Step:  170 | Epoch: 2.79 | Loss: 0.1371\n",
      "üìù Step:  180 | Epoch: 2.96 | Loss: 0.1377\n",
      "üìù Step:  190 | Epoch: 3.12 | Loss: 0.1499\n",
      "üìù Step:  200 | Epoch: 3.28 | Loss: 0.1238\n",
      "üìù Step:  210 | Epoch: 3.45 | Loss: 0.1295\n",
      "üìù Step:  220 | Epoch: 3.61 | Loss: 0.1188\n",
      "üìù Step:  230 | Epoch: 3.78 | Loss: 0.1402\n",
      "üìù Step:  240 | Epoch: 3.94 | Loss: 0.1318\n",
      "üìù Step:  250 | Epoch: 4.10 | Loss: 0.1324\n",
      "üìù Step:  260 | Epoch: 4.26 | Loss: 0.1181\n",
      "üìù Step:  270 | Epoch: 4.43 | Loss: 0.1301\n",
      "üìù Step:  280 | Epoch: 4.60 | Loss: 0.1187\n",
      "üìù Step:  290 | Epoch: 4.76 | Loss: 0.1290\n",
      "üìù Step:  300 | Epoch: 4.93 | Loss: 0.1428\n",
      "‚úÖ Training M2 Finito. Durata: 150.59 min | Loss: 0.4806\n",
      "üíæ Salvataggio Artifacts M2...\n",
      "üì¶ Compressione ZIP in corso (attendere)...\n",
      "   -> ZIP creato: outputs/Qwen2.5-VL-M2-Classification_Seed_92_FULL_CHECKPOINTS.zip\n",
      "üßπ Pulizia VRAM per il prossimo seed...\n",
      "‚ú® Ambiente pulito.\n",
      "\n",
      "\n",
      "üéâ TUTTE LE RUN PER M2 SONO COMPLETATE CON SUCCESSO!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import shutil\n",
    "import time\n",
    "import random \n",
    "import numpy as np \n",
    "import unsloth\n",
    "from datetime import datetime\n",
    "from datasets import load_from_disk\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from unsloth import FastVisionModel, UnslothVisionDataCollator, is_bfloat16_supported\n",
    "from transformers import TrainerCallback, set_seed \n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURAZIONE GLOBALE (Fissa per tutte le run)\n",
    "# ==============================================================================\n",
    "SEEDS = [101, 285, 3692, 92]  # <--- LISTA DI SEED DA TESTARE\n",
    "NUM_EPOCHS = 5                # <--- NUMERO DI EPOCHE PER OGNI RUN (con SEED X)\n",
    "MODEL_ID = \"unsloth/Qwen2.5-VL-7B-Instruct-bnb-4bit\"\n",
    "MODEL_SHORTNAME = \"Qwen2.5-VL-M2-Classification\"\n",
    "DATASET_PATH = os.path.join(\"DATASET_ITA\", \"PROCESSED_DATA\", \"HF_DATASETS\", \"M2_classification\") # <--- CAMBIATO DATASET\n",
    "\n",
    "# SYSTEM PROMPT (SPECIFICO PER M2 - CLASSIFICAZIONE 4 CLASSI)\n",
    "SYSTEM_INSTRUCTION_M2 = \"\"\"Sei un classificatore esperto specializzato nella tipologia di contenuti offensivi online.\n",
    "Il contenuto che analizzerai (testo del commento e frame del video) √® GIA' stato identificato come offensivo.\n",
    "Il tuo compito √® classificare ESATTAMENTE il tipo di offesa in una delle seguenti 4 categorie:\n",
    "\n",
    "1. FLAMING: Insulti diretti, linguaggio ostile, aggressivit√† verbale, minacce, uso di parolacce contro una persona.\n",
    "2. DENIGRATION: Attacchi alla reputazione, ridicolizzazione, svalutazione, diffamazione o umiliazione pubblica.\n",
    "3. SEXUAL: Molestie sessuali, commenti lascivi, oggettivazione sessuale, riferimenti espliciti non consensuali.\n",
    "4. RACISM: Discriminazione, stereotipi o insulti basati su razza, etnia, nazionalit√†, religione o colore della pelle.\n",
    "\n",
    "Analizza CONGIUNTAMENTE il testo del commento e i frame del video.\n",
    "Scegli la categoria che meglio descrive l'offesa predominante.\n",
    "Se pi√π categorie sono presenti, scegli quella DOMINANTE.\n",
    "\n",
    "Formato di Output OBBLIGATORIO:\n",
    "Rispondi SOLAMENTE con il numero della classe (1, 2, 3 o 4).\n",
    "Non aggiungere spiegazioni, punteggiatura o testo extra.\"\"\"\n",
    "\n",
    "# Callback per monitoraggio\n",
    "class RealTimePrinterCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and \"loss\" in logs:\n",
    "            print(f\"üìù Step: {state.global_step:4d} | Epoch: {logs['epoch']:.2f} | Loss: {logs['loss']:.4f}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CARICAMENTO E FORMATTAZIONE DATASET (Una volta sola per efficienza)\n",
    "# ==============================================================================\n",
    "print(\"üìÇ Caricamento Dataset HF M2 (Eseguito una volta sola)...\")\n",
    "dataset_raw = load_from_disk(DATASET_PATH)\n",
    "\n",
    "def has_valid_images(sample):\n",
    "    user_msg = sample[\"messages\"][0]\n",
    "    for item in user_msg[\"content\"]:\n",
    "        if item[\"type\"] == \"image\":\n",
    "            raw_path = item[\"image\"]\n",
    "            clean_path = raw_path.replace(\"file://\", \"\")\n",
    "            check_path = \"/\" + clean_path.lstrip(\"/\") if clean_path else \"\"\n",
    "            if not os.path.exists(check_path):\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "# Filtriamo eventuali immagini rotte\n",
    "train_valid = dataset_raw[\"train\"].filter(has_valid_images, desc=\"Filter Valid Imgs\")\n",
    "val_valid = dataset_raw[\"val\"].filter(has_valid_images, desc=\"Filter Valid Imgs\")\n",
    "\n",
    "def format_multimodal_sample_m2(sample):\n",
    "    raw_user_msg = sample[\"messages\"][0]\n",
    "    raw_assistant_msg = sample[\"messages\"][1] \n",
    "    user_content = []\n",
    "    \n",
    "    for item in raw_user_msg[\"content\"]:\n",
    "        if item[\"type\"] == \"image\":\n",
    "            raw_path = item[\"image\"]\n",
    "            clean_path = raw_path.replace(\"file://\", \"\")\n",
    "            clean_path = \"/\" + clean_path.lstrip(\"/\") \n",
    "            final_path = f\"file://{clean_path}\"\n",
    "            user_content.append({\"type\": \"image\", \"image\": final_path})\n",
    "        elif item[\"type\"] == \"text\":\n",
    "            text_clean = item[\"text\"].replace(\"Commento:\", \"\").strip().strip('\"').strip(\"'\")\n",
    "            text_final = f\"Commento: \\\"{text_clean}\\\"\"\n",
    "            user_content.append({\"type\": \"text\", \"text\": text_final})\n",
    "            \n",
    "    # Per M2 la label sar√† \"1\", \"2\", \"3\" o \"4\"\n",
    "    label_text = raw_assistant_msg[\"content\"][0][\"text\"]\n",
    "\n",
    "    new_messages = [\n",
    "        # QUI USIAMO IL PROMPT M2\n",
    "        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": SYSTEM_INSTRUCTION_M2}]},\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "        {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": label_text}]}\n",
    "    ]\n",
    "    return {\"messages\": new_messages}\n",
    "\n",
    "print(\"üîÑ Formattazione Dataset M2...\")\n",
    "train_dataset = train_valid.map(format_multimodal_sample_m2, batched=False, desc=\"Formatting Train M2\")\n",
    "val_dataset = val_valid.map(format_multimodal_sample_m2, batched=False, desc=\"Formatting Val M2\")\n",
    "print(f\"‚úÖ Dataset M2 Caricato e Formattato. Train: {len(train_dataset)} | Val: {len(val_dataset)}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. MEGA-LOOP DI TRAINING (TUTTI I SEEDS)\n",
    "# ==============================================================================\n",
    "print(f\"\\nüöÄ AVVIO SESSIONE DI TRAINING M2 SU {len(SEEDS)} SEED: {SEEDS}\")\n",
    "\n",
    "for seed_idx, TRAINING_SEED in enumerate(SEEDS):\n",
    "    print(\"\\n\" + \"#\"*60)\n",
    "    print(f\"üé¨ RUN {seed_idx + 1}/{len(SEEDS)} | SEED CORRENTE: {TRAINING_SEED}\")\n",
    "    print(\"#\"*60)\n",
    "\n",
    "    # --- üîí FIX DETERMINISMO GLOBALE ---\n",
    "    print(f\"üîí Fissaggio Seed Globali a {TRAINING_SEED}...\")\n",
    "    random.seed(TRAINING_SEED)\n",
    "    np.random.seed(TRAINING_SEED)\n",
    "    torch.manual_seed(TRAINING_SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(TRAINING_SEED)\n",
    "    set_seed(TRAINING_SEED)\n",
    "    # -------------------------------------------------------\n",
    "\n",
    "    # Definizione Output Directory Dinamica (M2)\n",
    "    OUTPUT_DIR = f\"outputs/{MODEL_SHORTNAME}_Seed_{TRAINING_SEED}\"\n",
    "    print(f\"üìÇ Cartella Output Run: {OUTPUT_DIR}\")\n",
    "\n",
    "    # --- A. CARICAMENTO MODELLO ---\n",
    "    print(f\"‚è≥ Inizializzazione Modello M2 (Seed {TRAINING_SEED})...\")\n",
    "    model, tokenizer = FastVisionModel.from_pretrained(\n",
    "        model_name = MODEL_ID,\n",
    "        load_in_4bit = True,\n",
    "        use_gradient_checkpointing = \"unsloth\",\n",
    "    )\n",
    "\n",
    "    model = FastVisionModel.get_peft_model(\n",
    "        model,\n",
    "        r = 16,\n",
    "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        lora_alpha = 16,\n",
    "        lora_dropout = 0,\n",
    "        bias = \"none\",\n",
    "        random_state = TRAINING_SEED, \n",
    "        use_rslora = False,\n",
    "        loftq_config = None,\n",
    "    )\n",
    "    FastVisionModel.for_training(model)\n",
    "\n",
    "    # --- B. CONFIGURAZIONE TRAINER ---\n",
    "    training_args = SFTConfig(\n",
    "        per_device_train_batch_size = 4,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        num_train_epochs = NUM_EPOCHS,\n",
    "        learning_rate = 5e-5,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        warmup_ratio = 0.1,\n",
    "        weight_decay = 0.01,\n",
    "        optim = \"adamw_8bit\",\n",
    "        max_grad_norm = 0.3,\n",
    "        \n",
    "        # Salvataggio\n",
    "        eval_strategy = \"epoch\",\n",
    "        save_strategy = \"epoch\",\n",
    "        save_total_limit = None,\n",
    "        load_best_model_at_end = False,\n",
    "        metric_for_best_model = \"eval_loss\",\n",
    "        greater_is_better = False,\n",
    "        \n",
    "        # Hardware & Path\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        gradient_checkpointing = True,\n",
    "        logging_steps = 10,\n",
    "        output_dir = OUTPUT_DIR, \n",
    "        report_to = \"none\",\n",
    "        \n",
    "        # Unsloth\n",
    "        remove_unused_columns = False,\n",
    "        dataset_text_field = \"\",\n",
    "        dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
    "        seed = TRAINING_SEED, \n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model = model,\n",
    "        tokenizer = tokenizer,\n",
    "        data_collator = UnslothVisionDataCollator(model, tokenizer),\n",
    "        train_dataset = train_dataset,\n",
    "        eval_dataset = val_dataset,\n",
    "        args = training_args,\n",
    "        callbacks = [RealTimePrinterCallback()],\n",
    "    )\n",
    "\n",
    "    # --- C. ESECUZIONE TRAINING ---\n",
    "    print(f\"üî• Avvio Training M2 Seed {TRAINING_SEED}...\")\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    trainer_stats = trainer.train()\n",
    "    end_time = time.time()\n",
    "    \n",
    "    total_duration = (end_time - start_time) / 60\n",
    "    final_train_loss = trainer_stats.training_loss\n",
    "    global_steps_done = trainer_stats.global_step\n",
    "\n",
    "    print(f\"‚úÖ Training M2 Finito. Durata: {total_duration:.2f} min | Loss: {final_train_loss:.4f}\")\n",
    "\n",
    "    # --- D. SALVATAGGIO ---\n",
    "    ADAPTER_PATH = os.path.join(OUTPUT_DIR, \"final_adapter_latest\")\n",
    "    REPORT_FILENAME = f\"training_report_Seed_{TRAINING_SEED}.json\"\n",
    "    REPORT_PATH = os.path.join(OUTPUT_DIR, REPORT_FILENAME)\n",
    "    ZIP_FILENAME = f\"{MODEL_SHORTNAME}_Seed_{TRAINING_SEED}_FULL_CHECKPOINTS\"\n",
    "    \n",
    "    # Cartella Padre per lo ZIP\n",
    "    PARENT_DIR = os.path.dirname(OUTPUT_DIR)\n",
    "    ZIP_FULL_PATH = os.path.join(PARENT_DIR, ZIP_FILENAME)\n",
    "\n",
    "    os.makedirs(ADAPTER_PATH, exist_ok=True)\n",
    "    \n",
    "    print(f\"üíæ Salvataggio Artifacts M2...\")\n",
    "    model.save_pretrained(ADAPTER_PATH)\n",
    "    tokenizer.save_pretrained(ADAPTER_PATH)\n",
    "\n",
    "    # Report JSON\n",
    "    peft_config_data = \"N/A\"\n",
    "    try:\n",
    "        raw_config = getattr(model, \"peft_config\", None)\n",
    "        if isinstance(raw_config, dict) and raw_config.get(\"default\"):\n",
    "            peft_config_data = str(raw_config[\"default\"])\n",
    "    except: pass\n",
    "\n",
    "    full_report = {\n",
    "        \"1_META_INFO\": {\n",
    "            \"timestamp_end\": datetime.now().isoformat(),\n",
    "            \"model_shortname\": MODEL_SHORTNAME,\n",
    "            \"seed\": TRAINING_SEED,\n",
    "            \"task\": \"M2 Classification (1-4) - Training Loop\" # <--- TASK CORRETTO\n",
    "        },\n",
    "        \"4_TRAINING_PERFORMANCE\": {\n",
    "            \"total_duration_minutes\": total_duration,\n",
    "            \"final_training_loss\": final_train_loss,\n",
    "            \"global_steps\": global_steps_done,\n",
    "            \"epochs\": training_args.num_train_epochs\n",
    "        },\n",
    "        \"5_LORA_PARAMS\": peft_config_data,\n",
    "        \"6_SYSTEM_PROMPT\": SYSTEM_INSTRUCTION_M2, # <--- SALVIAMO PROMPT M2\n",
    "        \"7_ARTIFACTS\": {\n",
    "            \"checkpoints_location\": \"Inside ZIP archive\",\n",
    "            \"zip_path\": f\"{ZIP_FULL_PATH}.zip\"\n",
    "        },\n",
    "        \"8_FULL_LOG_HISTORY\": getattr(trainer.state, \"log_history\", [])\n",
    "    }\n",
    "\n",
    "    with open(REPORT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(full_report, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"üì¶ Compressione ZIP in corso (attendere)...\")\n",
    "    shutil.make_archive(\n",
    "        base_name=ZIP_FULL_PATH, \n",
    "        format='zip', \n",
    "        root_dir=OUTPUT_DIR\n",
    "    )\n",
    "    print(f\"   -> ZIP creato: {ZIP_FULL_PATH}.zip\")\n",
    "\n",
    "    # --- E. PULIZIA MEMORIA ---\n",
    "    print(f\"üßπ Pulizia VRAM per il prossimo seed...\")\n",
    "    try:\n",
    "        del model\n",
    "        del trainer\n",
    "        del tokenizer\n",
    "    except: pass\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"‚ú® Ambiente pulito.\\n\")\n",
    "\n",
    "print(\"\\nüéâ TUTTE LE RUN PER M2 SONO COMPLETATE CON SUCCESSO!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2979dc0-4f49-4ef1-9df0-c8daba76f05b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformers (Python 3.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
